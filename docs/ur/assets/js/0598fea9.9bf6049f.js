"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[560],{5768:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>T,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4/object-detection-manipulation","title":"object-detection-manipulation","description":"MYMEMORY WARNING//MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/object-detection-manipulation.md","sourceDirName":"module-4","slug":"/module-4/object-detection-manipulation","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ur/docs/module-4/object-detection-manipulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Mrsaleem110/Physical-AI-humanoid-robotic-chatbot-book/tree/main/docs/docs/module-4/object-detection-manipulation.md","tags":[],"version":"current","lastUpdatedBy":"muhammad_saleem","lastUpdatedAt":1766408575000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"llm-cognitive-planning","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ur/docs/module-4/llm-cognitive-planning"},"next":{"title":"capstone-autonomous-humanoid","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ur/docs/module-4/capstone-autonomous-humanoid"}}');var i=r(4848),s=r(8453);const a={sidebar_position:3},o=void 0,c={},l=[];function p(e){const n={a:"a",code:"code",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 47 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 47 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 47 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 46 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 46 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 45 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 45 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 44 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 44 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 43 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 43 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 43 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 42 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 42 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 41 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 41 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 40 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 40 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 40 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 39 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 39 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 38 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 38 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 37 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 37 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 36 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 36 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 36 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# python/yolo_robotics.py\r\nimport torch\r\nimport torchvision\r\nfrom torchvision import transforms\r\nimport cv2\r\nimport numpy as np\r\nfrom typing import List, Tuple, Dict, Optional\r\nimport time\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass DetectionResult:\r\n    \"\"\"Result of object detection\"\"\"\r\n    class_id: int\r\n    class_name: str\r\n    confidence: float\r\n    bbox: Tuple[int, int, int, int]  # (x, y, width, height)\r\n    center_3d: Optional[Tuple[float, float, float]]  # 3D position in world coordinates\r\n    rotation_3d: Optional[Tuple[float, float, float]]  # 3D rotation\r\n\r\nclass YOLORobotDetector:\r\n    \"\"\"YOLO-based object detector optimized for robotics\"\"\"\r\n\r\n    def __init__(self, model_path: str = \"yolov5s.pt\", device: str = \"cuda\"):\r\n        self.device = device if torch.cuda.is_available() and device == \"cuda\" else \"cpu\"\r\n\r\n        # Load YOLO model\r\n        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\r\n        self.model.to(self.device)\r\n        self.model.eval()\r\n\r\n        # Set model parameters\r\n        self.confidence_threshold = 0.5\r\n        self.iou_threshold = 0.4\r\n        self.image_size = 640  # Standard YOLO input size\r\n\r\n        # Class names (COCO dataset by default)\r\n        self.class_names = [\r\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\r\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\r\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\r\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\r\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\r\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\r\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\r\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\r\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\r\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\r\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\r\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\r\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\r\n        ]\r\n\r\n        # Camera intrinsic parameters (should be calibrated for your specific camera)\r\n        self.camera_matrix = np.array([\r\n            [554.25, 0.0, 320.0],\r\n            [0.0, 554.25, 240.0],\r\n            [0.0, 0.0, 1.0]\r\n        ])\r\n\r\n        print(f\"YOLO Robot Detector initialized on {self.device}\")\r\n\r\n    def detect_objects(self, image: np.ndarray) -> List[DetectionResult]:\r\n        \"\"\"Detect objects in an image\"\"\"\r\n        start_time = time.time()\r\n\r\n        # Preprocess image\r\n        img_tensor = self._preprocess_image(image)\r\n\r\n        # Run inference\r\n        with torch.no_grad():\r\n            results = self.model(img_tensor)\r\n\r\n        # Process results\r\n        detections = self._process_detections(results, image.shape[:2])\r\n\r\n        # Calculate 3D positions if depth information is available\r\n        if hasattr(self, 'depth_image') and self.depth_image is not None:\r\n            detections = self._calculate_3d_positions(detections)\r\n\r\n        # Performance metrics\r\n        inference_time = time.time() - start_time\r\n        print(f\"Detection completed in {inference_time:.3f}s, found {len(detections)} objects\")\r\n\r\n        return detections\r\n\r\n    def _preprocess_image(self, image: np.ndarray) -> torch.Tensor:\r\n        \"\"\"Preprocess image for YOLO inference\"\"\"\r\n        # Convert BGR to RGB if needed\r\n        if len(image.shape) == 3 and image.shape[2] == 3:\r\n            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n        else:\r\n            image_rgb = image\r\n\r\n        # Resize image to model input size while maintaining aspect ratio\r\n        h, w = image_rgb.shape[:2]\r\n        scale = min(self.image_size / h, self.image_size / w)\r\n        new_h, new_w = int(h * scale), int(w * scale)\r\n\r\n        resized = cv2.resize(image_rgb, (new_w, new_h))\r\n\r\n        # Pad to make it square\r\n        delta_w = self.image_size - new_w\r\n        delta_h = self.image_size - new_h\r\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\r\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\r\n\r\n        padded = cv2.copyMakeBorder(\r\n            resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[114, 114, 114]\r\n        )\r\n\r\n        # Convert to tensor and normalize\r\n        img_tensor = torch.from_numpy(padded).permute(2, 0, 1).float() / 255.0\r\n        img_tensor = img_tensor.unsqueeze(0).to(self.device)\r\n\r\n        return img_tensor\r\n\r\n    def _process_detections(self, results, image_shape) -> List[DetectionResult]:\r\n        \"\"\"Process YOLO detection results\"\"\"\r\n        detections = []\r\n\r\n        # Get predictions\r\n        pred = results.pred[0]  # First image in batch\r\n\r\n        if len(pred) > 0:\r\n            for *xyxy, conf, cls in pred.tolist():\r\n                # Convert to integer coordinates\r\n                x1, y1, x2, y2 = map(int, xyxy)\r\n\r\n                # Calculate width and height\r\n                width = x2 - x1\r\n                height = y2 - y1\r\n\r\n                # Apply confidence threshold\r\n                if conf >= self.confidence_threshold:\r\n                    detection = DetectionResult(\r\n                        class_id=int(cls),\r\n                        class_name=self.class_names[int(cls)] if int(cls) < len(self.class_names) else f\"unknown_{int(cls)}\",\r\n                        confidence=conf,\r\n                        bbox=(x1, y1, width, height),\r\n                        center_3d=None,  # Will be calculated later if depth is available\r\n                        rotation_3d=None\r\n                    )\r\n                    detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def _calculate_3d_positions(self, detections: List[DetectionResult]) -> List[DetectionResult]:\r\n        \"\"\"Calculate 3D positions from 2D detections and depth image\"\"\"\r\n        updated_detections = []\r\n\r\n        for detection in detections:\r\n            # Get center of bounding box in 2D\r\n            x, y, w, h = detection.bbox\r\n            center_x = x + w // 2\r\n            center_y = y + h // 2\r\n\r\n            # Get depth at center point\r\n            if (center_y < self.depth_image.shape[0] and\r\n                center_x < self.depth_image.shape[1]):\r\n                depth = self.depth_image[center_y, center_x]\r\n\r\n                # Convert 2D point to 3D using camera intrinsics\r\n                z = depth  # Depth value\r\n                x_3d = (center_x - self.camera_matrix[0, 2]) * z / self.camera_matrix[0, 0]\r\n                y_3d = (center_y - self.camera_matrix[1, 2]) * z / self.camera_matrix[1, 1]\r\n\r\n                detection.center_3d = (x_3d, y_3d, z)\r\n\r\n            updated_detections.append(detection)\r\n\r\n        return updated_detections\r\n\r\n    def set_depth_image(self, depth_image: np.ndarray):\r\n        \"\"\"Set depth image for 3D position calculation\"\"\"\r\n        self.depth_image = depth_image\r\n\r\n    def visualize_detections(self, image: np.ndarray, detections: List[DetectionResult]) -> np.ndarray:\r\n        \"\"\"Visualize detections on image\"\"\"\r\n        vis_image = image.copy()\r\n\r\n        for detection in detections:\r\n            x, y, w, h = detection.bbox\r\n\r\n            # Draw bounding box\r\n            cv2.rectangle(vis_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\r\n\r\n            # Draw label\r\n            label = f\"{detection.class_name}: {detection.confidence:.2f}\"\r\n            cv2.putText(vis_image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n\r\n            # Draw center point\r\n            center_x, center_y = x + w // 2, y + h // 2\r\n            cv2.circle(vis_image, (center_x, center_y), 5, (0, 0, 255), -1)\r\n\r\n        return vis_image\r\n\r\nclass RealTimeObjectDetector:\r\n    \"\"\"Real-time object detection for robotics applications\"\"\"\r\n\r\n    def __init__(self, detector: YOLORobotDetector):\r\n        self.detector = detector\r\n        self.is_running = False\r\n        self.detection_callback = None\r\n        self.frame_buffer = []\r\n        self.max_buffer_size = 5\r\n\r\n    def start_detection(self, callback: callable = None):\r\n        \"\"\"Start real-time detection\"\"\"\r\n        self.detection_callback = callback\r\n        self.is_running = True\r\n        print(\"Real-time object detection started\")\r\n\r\n    def stop_detection(self):\r\n        \"\"\"Stop real-time detection\"\"\"\r\n        self.is_running = False\r\n        print(\"Real-time object detection stopped\")\r\n\r\n    def process_frame(self, frame: np.ndarray) -> List[DetectionResult]:\r\n        \"\"\"Process a single frame\"\"\"\r\n        if not self.is_running:\r\n            return []\r\n\r\n        # Add frame to buffer\r\n        self.frame_buffer.append(frame)\r\n        if len(self.frame_buffer) > self.max_buffer_size:\r\n            self.frame_buffer.pop(0)  # Remove oldest frame\r\n\r\n        # Run detection\r\n        detections = self.detector.detect_objects(frame)\r\n\r\n        # Call callback if provided\r\n        if self.detection_callback:\r\n            self.detection_callback(detections, frame)\r\n\r\n        return detections\r\n\r\ndef demonstrate_yolo_detection():\r\n    \"\"\"Demonstrate YOLO object detection\"\"\"\r\n    print(\"Demonstrating YOLO Object Detection for Robotics\")\r\n\r\n    try:\r\n        # Initialize detector (this would load a pre-trained model)\r\n        # For this example, we'll just show the structure\r\n        detector = YOLORobotDetector(model_path=\"yolov5s.pt\")  # Replace with actual model path\r\n\r\n        # Example: Process a sample image\r\n        # In practice, you would load an actual image\r\n        sample_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n\r\n        detections = detector.detect_objects(sample_image)\r\n\r\n        print(f\"Found {len(detections)} objects:\")\r\n        for detection in detections[:5]:  # Show first 5 detections\r\n            print(f\"  - {detection.class_name} (confidence: {detection.confidence:.2f})\")\r\n\r\n    except Exception as e:\r\n        print(f\"Error initializing YOLO detector: {e}\")\r\n        print(\"Make sure you have YOLOv5 installed and model file available\")\r\n\r\nif __name__ == \"__main__\":\r\n    demonstrate_yolo_detection()\n"})}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 35 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 35 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# python/transformer_detection.py\r\nimport torch\r\nimport torch.nn as nn\r\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\r\nimport cv2\r\nimport numpy as np\r\nfrom typing import List, Dict, Any\r\nimport requests\r\nfrom PIL import Image\r\nimport time\r\n\r\nclass DETRRobotDetector:\r\n    """DETR (DEtection TRansformer) for robotic object detection"""\r\n\r\n    def __init__(self, model_name: str = "facebook/detr-resnet-50", device: str = "cuda"):\r\n        self.device = device if torch.cuda.is_available() and device == "cuda" else "cpu"\r\n\r\n        # Load DETR model and processor\r\n        self.processor = DetrImageProcessor.from_pretrained(model_name)\r\n        self.model = DetrForObjectDetection.from_pretrained(model_name)\r\n        self.model.to(self.device)\r\n        self.model.eval()\r\n\r\n        # COCO dataset categories\r\n        self.categories = [\r\n            "N/A", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",\r\n            "traffic light", "fire hydrant", "N/A", "stop sign", "parking meter", "bench", "bird", "cat",\r\n            "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "N/A", "backpack",\r\n            "umbrella", "N/A", "N/A", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",\r\n            "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",\r\n            "tennis racket", "bottle", "N/A", "wine glass", "cup", "fork", "knife", "spoon", "bowl",\r\n            "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut",\r\n            "cake", "chair", "couch", "potted plant", "bed", "N/A", "dining table", "N/A", "N/A",\r\n            "toilet", "N/A", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave",\r\n            "oven", "toaster", "sink", "refrigerator", "N/A", "book", "clock", "vase", "scissors",\r\n            "teddy bear", "hair drier", "toothbrush"\r\n        ]\r\n\r\n        self.confidence_threshold = 0.9\r\n\r\n        print(f"DETR Robot Detector initialized on {self.device}")\r\n\r\n    def detect_objects(self, image: np.ndarray) -> List[DetectionResult]:\r\n        """Detect objects using DETR"""\r\n        start_time = time.time()\r\n\r\n        # Convert numpy array to PIL Image\r\n        if isinstance(image, np.ndarray):\r\n            if len(image.shape) == 3:\r\n                image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n            else:\r\n                image_pil = Image.fromarray(image)\r\n        else:\r\n            image_pil = image\r\n\r\n        # Process image\r\n        inputs = self.processor(images=image_pil, return_tensors="pt")\r\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\r\n\r\n        # Run inference\r\n        with torch.no_grad():\r\n            outputs = self.model(**inputs)\r\n\r\n        # Convert outputs to detections\r\n        target_sizes = torch.tensor([image_pil.size[::-1]]).to(self.device)\r\n        results = self.processor.post_process_object_detection(\r\n            outputs, target_sizes=target_sizes, threshold=self.confidence_threshold\r\n        )[0]\r\n\r\n        # Process results\r\n        detections = []\r\n        for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):\r\n            detection = DetectionResult(\r\n                class_id=label.item(),\r\n                class_name=self.categories[label] if label < len(self.categories) else f"unknown_{label}",\r\n                confidence=score.item(),\r\n                bbox=(int(box[0].item()), int(box[1].item()),\r\n                      int(box[2].item() - box[0].item()), int(box[3].item() - box[1].item())),\r\n                center_3d=None,\r\n                rotation_3d=None\r\n            )\r\n            detections.append(detection)\r\n\r\n        inference_time = time.time() - start_time\r\n        print(f"DETR detection completed in {inference_time:.3f}s, found {len(detections)} objects")\r\n\r\n        return detections\r\n\r\nclass CustomObjectDetector(nn.Module):\r\n    """Custom object detection model for robotics-specific objects"""\r\n\r\n    def __init__(self, num_classes: int = 20, pretrained: bool = True):\r\n        super().__init__()\r\n\r\n        # Use a backbone network (ResNet50 as example)\r\n        backbone = torch.hub.load(\'pytorch/vision:v0.10.0\', \'resnet50\', pretrained=pretrained)\r\n\r\n        # Remove the final classification layer\r\n        backbone = torch.nn.Sequential(*list(backbone.children())[:-2])\r\n\r\n        # Feature pyramid network for multi-scale detection\r\n        self.backbone = backbone\r\n        self.fpn = self._build_fpn()\r\n\r\n        # Detection heads\r\n        self.classification_head = self._build_classification_head(num_classes)\r\n        self.regression_head = self._build_regression_head()\r\n\r\n        # Anchor generation\r\n        self.anchors = self._generate_anchors()\r\n\r\n    def _build_fpn(self):\r\n        """Build Feature Pyramid Network"""\r\n        # Simplified FPN implementation\r\n        return nn.ModuleList([\r\n            nn.Conv2d(2048, 256, kernel_size=1),  # Top-down pathway\r\n            nn.Conv2d(1024, 256, kernel_size=1),\r\n            nn.Conv2d(512, 256, kernel_size=1),\r\n        ])\r\n\r\n    def _build_classification_head(self, num_classes: int):\r\n        """Build classification head"""\r\n        return nn.Sequential(\r\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(256, num_classes, kernel_size=3, padding=1)\r\n        )\r\n\r\n    def _build_regression_head(self):\r\n        """Build bounding box regression head"""\r\n        return nn.Sequential(\r\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(256, 4, kernel_size=3, padding=1)  # 4 coordinates: x, y, w, h\r\n        )\r\n\r\n    def _generate_anchors(self):\r\n        """Generate anchor boxes"""\r\n        # This is a simplified anchor generation\r\n        # In practice, you\'d have multiple scales and aspect ratios\r\n        anchors = []\r\n        scales = [32, 64, 128, 256, 512]\r\n        ratios = [0.5, 1.0, 2.0]\r\n\r\n        for scale in scales:\r\n            for ratio in ratios:\r\n                w = scale * (ratio ** 0.5)\r\n                h = scale / (ratio ** 0.5)\r\n                anchors.append((w, h))\r\n\r\n        return anchors\r\n\r\n    def forward(self, x):\r\n        """Forward pass"""\r\n        # Extract features from backbone\r\n        features = self.backbone(x)\r\n\r\n        # Apply FPN\r\n        fpn_features = []\r\n        for i, layer in enumerate(self.fpn):\r\n            if i == 0:\r\n                fpn_features.append(layer(features))\r\n            else:\r\n                # Simplified top-down pathway\r\n                upsampled = torch.nn.functional.interpolate(\r\n                    fpn_features[-1], size=features.shape[-2:], mode=\'nearest\'\r\n                )\r\n                fpn_features.append(layer(features) + upsampled)\r\n\r\n        # Apply detection heads to each FPN level\r\n        classifications = []\r\n        regressions = []\r\n\r\n        for feat in fpn_features:\r\n            classifications.append(self.classification_head(feat))\r\n            regressions.append(self.regression_head(feat))\r\n\r\n        return classifications, regressions\r\n\r\nclass MultiModalDetector:\r\n    """Multi-modal object detection combining vision and other sensors"""\r\n\r\n    def __init__(self):\r\n        self.vision_detector = DETRRobotDetector()\r\n        self.fusion_weights = {\r\n            \'vision\': 0.7,\r\n            \'depth\': 0.2,\r\n            \'touch\': 0.1\r\n        }\r\n        self.confidence_threshold = 0.5\r\n\r\n    def detect_with_fusion(self, rgb_image: np.ndarray, depth_image: np.ndarray = None) -> List[DetectionResult]:\r\n        """Detect objects using multi-modal fusion"""\r\n        # Get vision-based detections\r\n        vision_detections = self.vision_detector.detect_objects(rgb_image)\r\n\r\n        # If depth image is available, refine detections\r\n        if depth_image is not None:\r\n            refined_detections = self._refine_with_depth(vision_detections, rgb_image, depth_image)\r\n        else:\r\n            refined_detections = vision_detections\r\n\r\n        # Filter based on confidence\r\n        final_detections = [\r\n            det for det in refined_detections\r\n            if det.confidence >= self.confidence_threshold\r\n        ]\r\n\r\n        return final_detections\r\n\r\n    def _refine_with_depth(self, detections: List[DetectionResult], rgb_image: np.ndarray, depth_image: np.ndarray) -> List[DetectionResult]:\r\n        """Refine detections using depth information"""\r\n        refined_detections = []\r\n\r\n        for detection in detections:\r\n            # Get bounding box center\r\n            x, y, w, h = detection.bbox\r\n            center_x, center_y = x + w // 2, y + h // 2\r\n\r\n            # Get depth at center point\r\n            if center_y < depth_image.shape[0] and center_x < depth_image.shape[1]:\r\n                depth = depth_image[center_y, center_x]\r\n\r\n                # Refine confidence based on depth consistency\r\n                # Objects with reasonable depth values get higher confidence\r\n                if 0.1 < depth < 5.0:  # Reasonable depth range for robotics\r\n                    refined_confidence = min(1.0, detection.confidence * 1.2)  # Boost confidence\r\n                else:\r\n                    refined_confidence = max(0.1, detection.confidence * 0.8)  # Reduce confidence\r\n\r\n                # Update detection with refined confidence and 3D position\r\n                detection.confidence = refined_confidence\r\n                detection.center_3d = self._get_3d_position(x, y, depth)\r\n\r\n            refined_detections.append(detection)\r\n\r\n        return refined_detections\r\n\r\n    def _get_3d_position(self, x: int, y: int, depth: float) -> Tuple[float, float, float]:\r\n        """Convert 2D pixel coordinates + depth to 3D world coordinates"""\r\n        # This is a simplified version - in practice you\'d use calibrated camera parameters\r\n        # Assuming camera intrinsics: fx=554.25, fy=554.25, cx=320, cy=240\r\n        fx, fy, cx, cy = 554.25, 554.25, 320.0, 240.0\r\n\r\n        z = depth\r\n        x_3d = (x - cx) * z / fx\r\n        y_3d = (y - cy) * z / fy\r\n\r\n        return (x_3d, y_3d, z)\r\n\r\ndef compare_detection_methods():\r\n    """Compare different detection methods"""\r\n    print("Comparing Object Detection Methods for Robotics")\r\n\r\n    # Initialize detectors\r\n    detr_detector = DETRRobotDetector()\r\n\r\n    print("\\nDETR Detector initialized")\r\n    print("Testing with sample image...")\r\n\r\n    # Create a sample image for testing\r\n    sample_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n\r\n    # Test DETR\r\n    start_time = time.time()\r\n    detr_detections = detr_detector.detect_objects(sample_image)\r\n    detr_time = time.time() - start_time\r\n\r\n    print(f"DETR: {len(detr_detections)} objects detected in {detr_time:.3f}s")\r\n\r\ndef demonstrate_transformer_detection():\r\n    """Demonstrate transformer-based detection"""\r\n    print("Demonstrating Transformer-Based Object Detection")\r\n\r\n    try:\r\n        # Initialize DETR detector\r\n        detector = DETRRobotDetector()\r\n\r\n        # Example usage would be:\r\n        # image = cv2.imread("path_to_image.jpg")\r\n        # detections = detector.detect_objects(image)\r\n\r\n        print("DETR detector ready for object detection")\r\n        print("Model: facebook/detr-resnet-50")\r\n        print("Classes: COCO dataset (91 classes)")\r\n\r\n    except Exception as e:\r\n        print(f"Error initializing DETR detector: {e}")\r\n        print("Make sure transformers and torch are properly installed")\r\n\r\nif __name__ == "__main__":\r\n    demonstrate_transformer_detection()\r\n    compare_detection_methods()\n'})}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 34 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 34 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# python/point_cloud_detection.py\r\nimport open3d as o3d\r\nimport numpy as np\r\nimport cv2\r\nfrom typing import List, Tuple, Dict, Optional\r\nimport time\r\nfrom sklearn.cluster import DBSCAN\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass PointCloudObjectDetector:\r\n    """Object detection using 3D point cloud data"""\r\n\r\n    def __init__(self):\r\n        self.voxel_size = 0.01  # 1cm voxel size\r\n        self.cluster_eps = 0.02  # 2cm clustering distance\r\n        self.min_points = 10     # Minimum points for a cluster\r\n        self.table_height = 0.0  # Assumed table height for ground removal\r\n\r\n    def detect_objects_from_pointcloud(self, pointcloud: np.ndarray) -> List[Dict]:\r\n        """Detect objects from 3D point cloud"""\r\n        start_time = time.time()\r\n\r\n        # Convert to Open3D point cloud\r\n        pcd = o3d.geometry.PointCloud()\r\n        pcd.points = o3d.utility.Vector3dVector(pointcloud)\r\n\r\n        # Downsample point cloud for efficiency\r\n        downsampled_pcd = pcd.voxel_down_sample(voxel_size=self.voxel_size)\r\n\r\n        # Remove ground plane (table surface)\r\n        objects_pcd = self._remove_ground_plane(downsampled_pcd)\r\n\r\n        # Segment objects using clustering\r\n        object_clusters = self._segment_objects(objects_pcd)\r\n\r\n        # Analyze each cluster to extract object information\r\n        objects = []\r\n        for i, cluster_indices in enumerate(object_clusters):\r\n            if len(cluster_indices) >= self.min_points:\r\n                cluster_points = np.asarray(objects_pcd.select_by_index(cluster_indices).points)\r\n\r\n                # Calculate object properties\r\n                centroid = np.mean(cluster_points, axis=0)\r\n                bbox = self._calculate_bounding_box(cluster_points)\r\n                dimensions = self._calculate_dimensions(cluster_points)\r\n\r\n                object_info = {\r\n                    \'id\': i,\r\n                    \'centroid\': centroid,\r\n                    \'bbox\': bbox,\r\n                    \'dimensions\': dimensions,\r\n                    \'point_count\': len(cluster_points),\r\n                    \'convex_hull\': self._calculate_convex_hull(cluster_points)\r\n                }\r\n\r\n                objects.append(object_info)\r\n\r\n        detection_time = time.time() - start_time\r\n        print(f"Point cloud detection completed in {detection_time:.3f}s, found {len(objects)} objects")\r\n\r\n        return objects\r\n\r\n    def _remove_ground_plane(self, pcd):\r\n        """Remove ground plane using RANSAC"""\r\n        # Segment the largest plane (assumed to be ground/table)\r\n        plane_model, inliers = pcd.segment_plane(\r\n            distance_threshold=0.01,\r\n            ransac_n=3,\r\n            num_iterations=1000\r\n        )\r\n\r\n        # Remove the ground plane points\r\n        objects_pcd = pcd.select_by_index(inliers, invert=True)\r\n\r\n        return objects_pcd\r\n\r\n    def _segment_objects(self, pcd):\r\n        """Segment individual objects using DBSCAN clustering"""\r\n        points = np.asarray(pcd.points)\r\n\r\n        # Perform clustering\r\n        clustering = DBSCAN(eps=self.cluster_eps, min_samples=2).fit(points)\r\n        labels = clustering.labels_\r\n\r\n        # Group points by cluster\r\n        unique_labels = set(labels)\r\n        clusters = []\r\n\r\n        for label in unique_labels:\r\n            if label == -1:  # Noise points\r\n                continue\r\n\r\n            cluster_indices = np.where(labels == label)[0]\r\n            clusters.append(cluster_indices)\r\n\r\n        return clusters\r\n\r\n    def _calculate_bounding_box(self, points: np.ndarray) -> Dict:\r\n        """Calculate 3D bounding box for a point cluster"""\r\n        min_vals = np.min(points, axis=0)\r\n        max_vals = np.max(points, axis=0)\r\n\r\n        center = (min_vals + max_vals) / 2\r\n        size = max_vals - min_vals\r\n\r\n        return {\r\n            \'center\': center,\r\n            \'size\': size,\r\n            \'min\': min_vals,\r\n            \'max\': max_vals\r\n        }\r\n\r\n    def _calculate_dimensions(self, points: np.ndarray) -> np.ndarray:\r\n        """Calculate object dimensions using PCA"""\r\n        # Center the points\r\n        centered_points = points - np.mean(points, axis=0)\r\n\r\n        # Calculate covariance matrix\r\n        cov_matrix = np.cov(centered_points.T)\r\n\r\n        # Calculate eigenvalues and eigenvectors\r\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\r\n\r\n        # Sort by eigenvalues (largest first)\r\n        idx = np.argsort(eigenvalues)[::-1]\r\n        eigenvalues = eigenvalues[idx]\r\n        eigenvectors = eigenvectors[:, idx]\r\n\r\n        # Dimensions are proportional to sqrt of eigenvalues\r\n        dimensions = np.sqrt(eigenvalues) * 2  # Multiply by 2 for full extent\r\n\r\n        return dimensions\r\n\r\n    def _calculate_convex_hull(self, points: np.ndarray) -> np.ndarray:\r\n        """Calculate convex hull of point cloud"""\r\n        try:\r\n            from scipy.spatial import ConvexHull\r\n            hull = ConvexHull(points)\r\n            return hull.vertices\r\n        except:\r\n            # Return all points if convex hull fails\r\n            return np.arange(len(points))\r\n\r\nclass RGBDObjectDetector:\r\n    """Object detection using RGB-D data (combining color and depth)"""\r\n\r\n    def __init__(self):\r\n        self.depth_threshold = 2.0  # Maximum depth to consider\r\n        self.min_object_size = 100  # Minimum number of pixels for an object\r\n        self.color_threshold = 30   # Color difference threshold for segmentation\r\n\r\n    def detect_objects_rgbd(self, rgb_image: np.ndarray, depth_image: np.ndarray) -> List[Dict]:\r\n        """Detect objects using RGB-D data"""\r\n        start_time = time.time()\r\n\r\n        # Convert to grayscale for processing\r\n        gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\r\n\r\n        # Create mask for valid depth regions\r\n        valid_depth_mask = (depth_image > 0.1) & (depth_image < self.depth_threshold)\r\n\r\n        # Apply depth-based segmentation\r\n        segmented_objects = self._segment_by_depth(depth_image, valid_depth_mask)\r\n\r\n        # Refine segments using color information\r\n        refined_objects = []\r\n        for obj_mask in segmented_objects:\r\n            # Combine depth and color segmentation\r\n            combined_mask = obj_mask & valid_depth_mask\r\n            if np.sum(combined_mask) >= self.min_object_size:\r\n                # Extract object properties\r\n                obj_info = self._extract_object_properties(\r\n                    rgb_image, depth_image, combined_mask\r\n                )\r\n                refined_objects.append(obj_info)\r\n\r\n        detection_time = time.time() - start_time\r\n        print(f"RGB-D detection completed in {detection_time:.3f}s, found {len(refined_objects)} objects")\r\n\r\n        return refined_objects\r\n\r\n    def _segment_by_depth(self, depth_image: np.ndarray, valid_mask: np.ndarray) -> List[np.ndarray]:\r\n        """Segment objects based on depth discontinuities"""\r\n        # Calculate depth gradients\r\n        grad_x = np.gradient(depth_image, axis=1)\r\n        grad_y = np.gradient(depth_image, axis=0)\r\n        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\r\n\r\n        # Create initial segmentation based on depth gradients\r\n        # Use watershed or region growing algorithm\r\n        from scipy import ndimage\r\n\r\n        # Mark regions with low gradient as potential objects\r\n        smooth_regions = grad_magnitude < 0.1  # Threshold for smooth regions\r\n        smooth_regions = smooth_regions & valid_mask\r\n\r\n        # Label connected components\r\n        labeled_regions, num_regions = ndimage.label(smooth_regions)\r\n\r\n        # Create masks for each region\r\n        object_masks = []\r\n        for i in range(1, num_regions + 1):\r\n            region_mask = (labeled_regions == i)\r\n            if np.sum(region_mask) >= self.min_object_size:\r\n                object_masks.append(region_mask)\r\n\r\n        return object_masks\r\n\r\n    def _extract_object_properties(self, rgb_image: np.ndarray, depth_image: np.ndarray, mask: np.ndarray) -> Dict:\r\n        """Extract properties of an object from RGB-D data"""\r\n        # Get object pixels\r\n        obj_rgb = rgb_image[mask]\r\n        obj_depth = depth_image[mask]\r\n\r\n        # Calculate 2D bounding box\r\n        coords = np.argwhere(mask)\r\n        min_row, min_col = coords.min(axis=0)\r\n        max_row, max_col = coords.max(axis=0)\r\n\r\n        # Calculate 3D centroid\r\n        y_coords, x_coords = coords.T\r\n        z_values = depth_image[y_coords, x_coords]\r\n\r\n        # Convert 2D coordinates to 3D using camera intrinsics\r\n        # Assuming camera intrinsics: fx=554.25, fy=554.25, cx=320, cy=240\r\n        fx, fy, cx, cy = 554.25, 554.25, 320.0, 240.0\r\n\r\n        x_3d = (x_coords - cx) * z_values / fx\r\n        y_3d = (y_coords - cy) * z_values / fy\r\n\r\n        centroid_3d = np.array([\r\n            np.mean(x_3d),\r\n            np.mean(y_3d),\r\n            np.mean(z_values)\r\n        ])\r\n\r\n        # Calculate object dimensions\r\n        dimensions_3d = np.array([\r\n            np.max(x_3d) - np.min(x_3d),\r\n            np.max(y_3d) - np.min(y_3d),\r\n            np.max(z_values) - np.min(z_values)\r\n        ])\r\n\r\n        # Calculate average color\r\n        avg_color = np.mean(obj_rgb, axis=0)\r\n\r\n        return {\r\n            \'bbox_2d\': (min_col, min_row, max_col - min_col, max_row - min_row),\r\n            \'centroid_3d\': centroid_3d,\r\n            \'dimensions_3d\': dimensions_3d,\r\n            \'avg_color\': avg_color,\r\n            \'pixel_count\': len(obj_rgb),\r\n            \'confidence\': self._calculate_detection_confidence(obj_depth, obj_rgb)\r\n        }\r\n\r\n    def _calculate_detection_confidence(self, depth_values: np.ndarray, color_values: np.ndarray) -> float:\r\n        """Calculate confidence score for object detection"""\r\n        # Confidence based on depth consistency and color uniformity\r\n        depth_std = np.std(depth_values)\r\n        color_std = np.std(color_values, axis=0).mean()\r\n\r\n        # Lower std = higher confidence\r\n        depth_conf = max(0.1, 1.0 - depth_std)  # Normalize to [0.1, 1.0]\r\n        color_conf = max(0.1, 1.0 - color_std / 255.0)  # Normalize color std\r\n\r\n        # Combine confidences\r\n        confidence = (depth_conf + color_conf) / 2.0\r\n        return min(1.0, confidence)\r\n\r\nclass ObjectPoseEstimator:\r\n    """Estimate 6D pose of objects for manipulation"""\r\n\r\n    def __init__(self):\r\n        self.template_models = {}  # Predefined object models\r\n        self.icp_threshold = 0.01  # ICP convergence threshold\r\n\r\n    def estimate_pose(self, detected_object: Dict, camera_matrix: np.ndarray) -> Dict:\r\n        """Estimate 6D pose of detected object"""\r\n        # This is a simplified pose estimation\r\n        # In practice, you\'d use template matching, PnP, or ICP\r\n\r\n        # For now, return a simple pose based on the detected properties\r\n        pose_info = {\r\n            \'position\': detected_object[\'centroid_3d\'],\r\n            \'orientation\': self._estimate_orientation(detected_object),  # Simple orientation estimation\r\n            \'confidence\': detected_object.get(\'confidence\', 0.8),\r\n            \'object_type\': self._classify_object_type(detected_object)\r\n        }\r\n\r\n        return pose_info\r\n\r\n    def _estimate_orientation(self, object_info: Dict) -> np.ndarray:\r\n        """Estimate object orientation from dimensions"""\r\n        # Simple heuristic: align longest dimension with Z-axis (upright)\r\n        dimensions = object_info[\'dimensions_3d\']\r\n        max_dim_idx = np.argmax(dimensions)\r\n\r\n        # Create a rotation matrix\r\n        rotation = np.eye(3)\r\n        # This is a simplified orientation - in practice you\'d use more sophisticated methods\r\n        return rotation.flatten()  # Return as 1D array\r\n\r\n    def _classify_object_type(self, object_info: Dict) -> str:\r\n        """Classify object type based on dimensions and other properties"""\r\n        dimensions = object_info[\'dimensions_3d\']\r\n        aspect_ratios = dimensions / np.min(dimensions)\r\n\r\n        if aspect_ratios[2] > 3:  # Tall object\r\n            return "tall_object"\r\n        elif aspect_ratios[0] > 2 and aspect_ratios[1] > 2:  # Flat object\r\n            return "flat_object"\r\n        elif np.all(aspect_ratios < 2):  # Cuboid-like\r\n            return "cuboid_object"\r\n        else:\r\n            return "irregular_object"\r\n\r\ndef demonstrate_point_cloud_detection():\r\n    """Demonstrate point cloud object detection"""\r\n    print("Demonstrating Point Cloud Object Detection")\r\n\r\n    # Initialize detector\r\n    detector = PointCloudObjectDetector()\r\n\r\n    # Create a sample point cloud (simulated)\r\n    # In practice, this would come from a depth sensor or RGB-D camera\r\n    np.random.seed(42)\r\n    # Simulate a table with some objects on it\r\n    table_points = np.random.uniform(low=[-0.5, -0.5, 0], high=[0.5, 0.5, 0.01], size=(1000, 3))\r\n    object1_points = np.random.uniform(low=[-0.2, -0.2, 0.01], high=[0.0, 0.0, 0.05], size=(200, 3))\r\n    object2_points = np.random.uniform(low=[0.1, 0.1, 0.01], high=[0.3, 0.3, 0.03], size=(150, 3))\r\n\r\n    full_pointcloud = np.vstack([table_points, object1_points, object2_points])\r\n\r\n    # Detect objects\r\n    objects = detector.detect_objects_from_pointcloud(full_pointcloud)\r\n\r\n    print(f"Detected {len(objects)} objects:")\r\n    for i, obj in enumerate(objects):\r\n        print(f"  Object {i+1}: centroid at {obj[\'centroid\']}, dimensions: {obj[\'dimensions\']}")\r\n\r\ndef demonstrate_rgbd_detection():\r\n    """Demonstrate RGB-D object detection"""\r\n    print("\\nDemonstrating RGB-D Object Detection")\r\n\r\n    # Initialize detector\r\n    detector = RGBDObjectDetector()\r\n\r\n    # Create sample RGB and depth images\r\n    rgb_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n    depth_image = np.random.uniform(0.5, 2.0, (480, 640)).astype(np.float32)\r\n\r\n    # Add some "objects" to the depth image\r\n    cv2.circle(depth_image, (100, 100), 30, 0.8, -1)  # Object 1\r\n    cv2.circle(depth_image, (300, 200), 25, 1.2, -1)  # Object 2\r\n    cv2.circle(depth_image, (500, 300), 40, 1.5, -1)  # Object 3\r\n\r\n    # Detect objects\r\n    objects = detector.detect_objects_rgbd(rgb_image, depth_image)\r\n\r\n    print(f"Detected {len(objects)} objects with RGB-D:")\r\n    for i, obj in enumerate(objects):\r\n        print(f"  Object {i+1}: 3D centroid at {obj[\'centroid_3d\']}, confidence: {obj[\'confidence\']:.2f}")\r\n\r\nif __name__ == "__main__":\r\n    demonstrate_point_cloud_detection()\r\n    demonstrate_rgbd_detection()\n'})}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 33 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 33 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# python/grasp_planning.py\r\nimport numpy as np\r\nfrom typing import List, Tuple, Dict, Optional\r\nimport time\r\nfrom dataclasses import dataclass\r\nfrom enum import Enum\r\n\r\nclass GraspType(Enum):\r\n    \"\"\"Types of grasps for different objects\"\"\"\r\n    PINCH = \"pinch\"\r\n    PALM = \"palm\"\r\n    LATERAL = \"lateral\"\r\n    SUCTION = \"suction\"\r\n    SPECIALIZED = \"specialized\"\r\n\r\n@dataclass\r\nclass GraspPose:\r\n    \"\"\"Represents a potential grasp pose\"\"\"\r\n    position: np.ndarray  # 3D position [x, y, z]\r\n    orientation: np.ndarray  # 4D quaternion [x, y, z, w]\r\n    grasp_type: GraspType\r\n    quality_score: float\r\n    approach_direction: np.ndarray  # 3D approach vector\r\n    width: float  # Required gripper width\r\n\r\nclass GraspPlanner:\r\n    \"\"\"Plan grasps for detected objects\"\"\"\r\n\r\n    def __init__(self):\r\n        self.min_grasp_quality = 0.3\r\n        self.max_grasp_attempts = 10\r\n        self.gripper_width_range = (0.02, 0.1)  # 2cm to 10cm\r\n\r\n    def plan_grasps(self, object_info: Dict, object_pose: Dict) -> List[GraspPose]:\r\n        \"\"\"Plan grasps for a detected object\"\"\"\r\n        start_time = time.time()\r\n\r\n        grasps = []\r\n\r\n        # Determine object type and plan appropriate grasps\r\n        obj_type = object_pose.get('object_type', 'irregular_object')\r\n        dimensions = object_info.get('dimensions_3d', np.array([0.1, 0.1, 0.1]))\r\n\r\n        if obj_type == \"tall_object\":\r\n            # Plan grasps for tall objects (cylindrical, bottles, etc.)\r\n            grasps.extend(self._plan_cylindrical_grasps(object_pose, dimensions))\r\n        elif obj_type == \"flat_object\":\r\n            # Plan grasps for flat objects (books, plates, etc.)\r\n            grasps.extend(self._plan_planar_grasps(object_pose, dimensions))\r\n        elif obj_type == \"cuboid_object\":\r\n            # Plan grasps for box-like objects\r\n            grasps.extend(self._plan_box_grasps(object_pose, dimensions))\r\n        else:\r\n            # Plan generic grasps for irregular objects\r\n            grasps.extend(self._plan_generic_grasps(object_pose, dimensions))\r\n\r\n        # Filter grasps by quality\r\n        filtered_grasps = [g for g in grasps if g.quality_score >= self.min_grasp_quality]\r\n\r\n        # Sort by quality score (highest first)\r\n        filtered_grasps.sort(key=lambda g: g.quality_score, reverse=True)\r\n\r\n        planning_time = time.time() - start_time\r\n        print(f\"Grasp planning completed in {planning_time:.3f}s, generated {len(filtered_grasps)} viable grasps\")\r\n\r\n        return filtered_grasps\r\n\r\n    def _plan_cylindrical_grasps(self, object_pose: Dict, dimensions: np.ndarray) -> List[GraspPose]:\r\n        \"\"\"Plan grasps for cylindrical objects\"\"\"\r\n        grasps = []\r\n\r\n        # Get object center and orientation\r\n        center = object_pose['position']\r\n        orientation = object_pose.get('orientation', np.array([0, 0, 0, 1]))\r\n\r\n        # Calculate grasp positions around the cylinder\r\n        radius = max(dimensions[0], dimensions[1]) / 2\r\n        height = dimensions[2]\r\n\r\n        # Plan circumferential grasps\r\n        num_grasps = 8\r\n        for i in range(num_grasps):\r\n            angle = 2 * np.pi * i / num_grasps\r\n\r\n            # Position grasp point at the radius\r\n            x_offset = radius * np.cos(angle)\r\n            y_offset = radius * np.sin(angle)\r\n\r\n            grasp_pos = np.array([\r\n                center[0] + x_offset,\r\n                center[1] + y_offset,\r\n                center[2] + height / 2  # Middle height\r\n            ])\r\n\r\n            # Calculate approach direction (radially inward)\r\n            approach_dir = -np.array([x_offset, y_offset, 0])\r\n            approach_dir = approach_dir / np.linalg.norm(approach_dir)\r\n\r\n            # Calculate orientation for pinch grasp\r\n            # Grasp along the cylinder axis\r\n            grasp_orientation = self._calculate_grasp_orientation(\r\n                approach_dir, np.array([0, 0, 1])  # Along Z axis\r\n            )\r\n\r\n            grasp_width = min(0.08, radius * 1.5)  # Adaptive gripper width\r\n\r\n            grasp = GraspPose(\r\n                position=grasp_pos,\r\n                orientation=grasp_orientation,\r\n                grasp_type=GraspType.PINCH,\r\n                quality_score=self._calculate_grasp_quality(grasp_pos, dimensions, GraspType.PINCH),\r\n                approach_direction=approach_dir,\r\n                width=grasp_width\r\n            )\r\n            grasps.append(grasp)\r\n\r\n        return grasps\r\n\r\n    def _plan_planar_grasps(self, object_pose: Dict, dimensions: np.ndarray) -> List[GraspPose]:\r\n        \"\"\"Plan grasps for planar/flat objects\"\"\"\r\n        grasps = []\r\n\r\n        center = object_pose['position']\r\n\r\n        # Plan edge grasps\r\n        thickness = min(dimensions)  # Smallest dimension is thickness\r\n        width = max(dimensions)      # Largest is width\r\n        length = np.sort(dimensions)[1]  # Middle is length\r\n\r\n        # Grasp along the four edges\r\n        edge_offsets = [\r\n            np.array([width/2, 0, thickness/2]),   # Right edge\r\n            np.array([-width/2, 0, thickness/2]),  # Left edge\r\n            np.array([0, length/2, thickness/2]),  # Top edge\r\n            np.array([0, -length/2, thickness/2])  # Bottom edge\r\n        ]\r\n\r\n        for offset in edge_offsets:\r\n            grasp_pos = center + offset\r\n\r\n            # Approach from above (perpendicular to the plane)\r\n            approach_dir = np.array([0, 0, -1])  # From above\r\n\r\n            # Calculate orientation for lateral grasp\r\n            grasp_orientation = self._calculate_grasp_orientation(\r\n                approach_dir, np.array([1, 0, 0])  # Gripper fingers horizontal\r\n            )\r\n\r\n            grasp_width = min(0.06, length * 0.7)  # Adaptive width\r\n\r\n            grasp = GraspPose(\r\n                position=grasp_pos,\r\n                orientation=grasp_orientation,\r\n                grasp_type=GraspType.LATERAL,\r\n                quality_score=self._calculate_grasp_quality(grasp_pos, dimensions, GraspType.LATERAL),\r\n                approach_direction=approach_dir,\r\n                width=grasp_width\r\n            )\r\n            grasps.append(grasp)\r\n\r\n        return grasps\r\n\r\n    def _plan_box_grasps(self, object_pose: Dict, dimensions: np.ndarray) -> List[GraspPose]:\r\n        \"\"\"Plan grasps for box-like objects\"\"\"\r\n        grasps = []\r\n\r\n        center = object_pose['position']\r\n\r\n        # Plan corner grasps\r\n        corner_offsets = [\r\n            np.array([dimensions[0]/2, dimensions[1]/2, 0]),   # Top-right corner\r\n            np.array([dimensions[0]/2, -dimensions[1]/2, 0]),  # Bottom-right corner\r\n            np.array([-dimensions[0]/2, dimensions[1]/2, 0]),  # Top-left corner\r\n            np.array([-dimensions[0]/2, -dimensions[1]/2, 0])  # Bottom-left corner\r\n        ]\r\n\r\n        for offset in corner_offsets:\r\n            grasp_pos = center + offset\r\n\r\n            # Approach from above\r\n            approach_dir = np.array([0, 0, -1])\r\n\r\n            # Calculate orientation for corner grasp\r\n            grasp_orientation = self._calculate_grasp_orientation(\r\n                approach_dir, np.array([0, 1, 0])  # Gripper fingers along Y\r\n            )\r\n\r\n            grasp_width = min(0.08, max(dimensions[0], dimensions[1]) * 0.8)\r\n\r\n            grasp = GraspPose(\r\n                position=grasp_pos,\r\n                orientation=grasp_orientation,\r\n                grasp_type=GraspType.PALM,\r\n                quality_score=self._calculate_grasp_quality(grasp_pos, dimensions, GraspType.PALM),\r\n                approach_direction=approach_dir,\r\n                width=grasp_width\r\n            )\r\n            grasps.append(grasp)\r\n\r\n        # Plan face grasps\r\n        face_centers = [\r\n            np.array([0, 0, dimensions[2]/2]),      # Top face\r\n            np.array([dimensions[0]/2, 0, 0]),      # Right face\r\n            np.array([-dimensions[0]/2, 0, 0]),     # Left face\r\n            np.array([0, dimensions[1]/2, 0]),      # Front face\r\n            np.array([0, -dimensions[1]/2, 0])      # Back face\r\n        ]\r\n\r\n        for offset in face_centers:\r\n            grasp_pos = center + offset\r\n\r\n            # Approach direction normal to the face\r\n            if abs(offset[2]) > 0:  # Top face\r\n                approach_dir = np.array([0, 0, -1])\r\n            elif abs(offset[0]) > 0:  # Side faces\r\n                approach_dir = np.array([-np.sign(offset[0]), 0, 0])\r\n            else:  # Front/back faces\r\n                approach_dir = np.array([0, -np.sign(offset[1]), 0])\r\n\r\n            # Calculate orientation for face grasp\r\n            grasp_orientation = self._calculate_grasp_orientation(\r\n                approach_dir, np.array([1, 0, 0])\r\n            )\r\n\r\n            grasp_width = min(0.08, max(dimensions[0], dimensions[1]) * 0.6)\r\n\r\n            grasp = GraspPose(\r\n                position=grasp_pos,\r\n                orientation=grasp_orientation,\r\n                grasp_type=GraspType.PALM,\r\n                quality_score=self._calculate_grasp_quality(grasp_pos, dimensions, GraspType.PALM),\r\n                approach_direction=approach_dir,\r\n                width=grasp_width\r\n            )\r\n            grasps.append(grasp)\r\n\r\n        return grasps\r\n\r\n    def _plan_generic_grasps(self, object_pose: Dict, dimensions: np.ndarray) -> List[GraspPose]:\r\n        \"\"\"Plan generic grasps for irregular objects\"\"\"\r\n        grasps = []\r\n\r\n        center = object_pose['position']\r\n\r\n        # Plan center grasp\r\n        grasp_pos = center + np.array([0, 0, max(dimensions) / 2])  # Above center\r\n\r\n        approach_dir = np.array([0, 0, -1])  # From above\r\n        grasp_orientation = self._calculate_grasp_orientation(\r\n            approach_dir, np.array([0, 1, 0])\r\n        )\r\n\r\n        grasp_width = min(0.1, np.mean(dimensions) * 2)\r\n\r\n        grasp = GraspPose(\r\n            position=grasp_pos,\r\n            orientation=grasp_orientation,\r\n            grasp_type=GraspType.PALM,\r\n            quality_score=self._calculate_grasp_quality(grasp_pos, dimensions, GraspType.PALM),\r\n            approach_direction=approach_dir,\r\n            width=grasp_width\r\n        )\r\n        grasps.append(grasp)\r\n\r\n        return grasps\r\n\r\n    def _calculate_grasp_orientation(self, approach_dir: np.ndarray, grasp_axis: np.ndarray) -> np.ndarray:\r\n        \"\"\"Calculate gripper orientation based on approach direction and grasp axis\"\"\"\r\n        # Normalize input vectors\r\n        approach = approach_dir / np.linalg.norm(approach_dir)\r\n        grasp_ax = grasp_axis / np.linalg.norm(grasp_axis)\r\n\r\n        # Calculate rotation to align z-axis with approach direction\r\n        z_axis = approach\r\n        y_axis = grasp_ax - np.dot(grasp_ax, z_axis) * z_axis\r\n        y_axis = y_axis / np.linalg.norm(y_axis) if np.linalg.norm(y_axis) > 0.001 else np.array([1, 0, 0])\r\n        x_axis = np.cross(y_axis, z_axis)\r\n\r\n        # Create rotation matrix\r\n        rotation_matrix = np.column_stack([x_axis, y_axis, z_axis])\r\n\r\n        # Convert to quaternion\r\n        quaternion = self._rotation_matrix_to_quaternion(rotation_matrix)\r\n\r\n        return quaternion\r\n\r\n    def _rotation_matrix_to_quaternion(self, rotation_matrix: np.ndarray) -> np.ndarray:\r\n        \"\"\"Convert rotation matrix to quaternion\"\"\"\r\n        # Method to convert rotation matrix to quaternion\r\n        trace = np.trace(rotation_matrix)\r\n\r\n        if trace > 0:\r\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\r\n            qw = 0.25 * s\r\n            qx = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\r\n            qy = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\r\n            qz = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\r\n        else:\r\n            if rotation_matrix[0, 0] > rotation_matrix[1, 1] and rotation_matrix[0, 0] > rotation_matrix[2, 2]:\r\n                s = np.sqrt(1.0 + rotation_matrix[0, 0] - rotation_matrix[1, 1] - rotation_matrix[2, 2]) * 2\r\n                qw = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\r\n                qx = 0.25 * s\r\n                qy = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\r\n                qz = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\r\n            elif rotation_matrix[1, 1] > rotation_matrix[2, 2]:\r\n                s = np.sqrt(1.0 + rotation_matrix[1, 1] - rotation_matrix[0, 0] - rotation_matrix[2, 2]) * 2\r\n                qw = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\r\n                qx = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\r\n                qy = 0.25 * s\r\n                qz = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\r\n            else:\r\n                s = np.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2\r\n                qw = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\r\n                qx = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\r\n                qy = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\r\n                qz = 0.25 * s\r\n\r\n        return np.array([qx, qy, qz, qw])\r\n\r\n    def _calculate_grasp_quality(self, position: np.ndarray, dimensions: np.ndarray, grasp_type: GraspType) -> float:\r\n        \"\"\"Calculate quality score for a grasp\"\"\"\r\n        # Quality factors:\r\n        # 1. Object size appropriateness\r\n        # 2. Grasp stability\r\n        # 3. Accessibility\r\n\r\n        size_factor = min(1.0, np.mean(dimensions) / 0.1)  # Normalize by typical object size\r\n        stability_factor = 0.8 if grasp_type in [GraspType.PALM, GraspType.PINCH] else 0.6\r\n        accessibility_factor = 0.9  # Assume good accessibility for now\r\n\r\n        quality = (size_factor * 0.4 + stability_factor * 0.4 + accessibility_factor * 0.2)\r\n        return min(1.0, max(0.1, quality))  # Clamp between 0.1 and 1.0\r\n\r\nclass ManipulationPlanner:\r\n    \"\"\"Plan complete manipulation sequences\"\"\"\r\n\r\n    def __init__(self):\r\n        self.grasp_planner = GraspPlanner()\r\n        self.max_retries = 3\r\n\r\n    def plan_manipulation(self, object_info: Dict, object_pose: Dict, target_pose: np.ndarray) -> Dict:\r\n        \"\"\"Plan complete manipulation sequence: approach, grasp, lift, transport, place\"\"\"\r\n        start_time = time.time()\r\n\r\n        # Plan grasps for the object\r\n        grasps = self.grasp_planner.plan_grasps(object_info, object_pose)\r\n\r\n        if not grasps:\r\n            return {\r\n                'success': False,\r\n                'error': 'No viable grasps found',\r\n                'actions': []\r\n            }\r\n\r\n        # Select the best grasp\r\n        best_grasp = grasps[0]\r\n\r\n        # Plan manipulation sequence\r\n        manipulation_sequence = self._create_manipulation_sequence(\r\n            object_pose['position'],\r\n            best_grasp,\r\n            target_pose\r\n        )\r\n\r\n        planning_time = time.time() - start_time\r\n        print(f\"Manipulation planning completed in {planning_time:.3f}s\")\r\n\r\n        return {\r\n            'success': True,\r\n            'grasp_pose': best_grasp,\r\n            'sequence': manipulation_sequence,\r\n            'planning_time': planning_time\r\n        }\r\n\r\n    def _create_manipulation_sequence(self, object_pos: np.ndarray, grasp: GraspPose, target_pos: np.ndarray) -> List[Dict]:\r\n        \"\"\"Create a complete manipulation sequence\"\"\"\r\n        sequence = []\r\n\r\n        # 1. Approach object\r\n        approach_pos = grasp.position + grasp.approach_direction * 0.1  # 10cm above grasp point\r\n        sequence.append({\r\n            'action': 'move_to',\r\n            'position': approach_pos,\r\n            'orientation': grasp.orientation,\r\n            'description': 'Approach object'\r\n        })\r\n\r\n        # 2. Descend to grasp\r\n        sequence.append({\r\n            'action': 'move_to',\r\n            'position': grasp.position,\r\n            'orientation': grasp.orientation,\r\n            'description': 'Move to grasp position'\r\n        })\r\n\r\n        # 3. Close gripper\r\n        sequence.append({\r\n            'action': 'grasp',\r\n            'width': grasp.width,\r\n            'description': 'Close gripper'\r\n        })\r\n\r\n        # 4. Lift object\r\n        lift_pos = grasp.position + np.array([0, 0, 0.1])  # Lift 10cm\r\n        sequence.append({\r\n            'action': 'move_to',\r\n            'position': lift_pos,\r\n            'orientation': grasp.orientation,\r\n            'description': 'Lift object'\r\n        })\r\n\r\n        # 5. Move to target\r\n        sequence.append({\r\n            'action': 'move_to',\r\n            'position': target_pos + np.array([0, 0, 0.1]),  # 10cm above target\r\n            'orientation': grasp.orientation,\r\n            'description': 'Move to target location'\r\n        })\r\n\r\n        # 6. Descend to place\r\n        place_pos = target_pos + np.array([0, 0, 0.05])  # 5cm above target\r\n        sequence.append({\r\n            'action': 'move_to',\r\n            'position': place_pos,\r\n            'orientation': grasp.orientation,\r\n            'description': 'Move to place position'\r\n        })\r\n\r\n        # 7. Open gripper\r\n        sequence.append({\r\n            'action': 'release',\r\n            'description': 'Open gripper'\r\n        })\r\n\r\n        # 8. Retract\r\n        retract_pos = place_pos + np.array([0, 0, 0.1])  # Retract 10cm\r\n        sequence.append({\r\n            'action': 'move_to',\r\n            'position': retract_pos,\r\n            'orientation': grasp.orientation,\r\n            'description': 'Retract gripper'\r\n        })\r\n\r\n        return sequence\r\n\r\ndef demonstrate_grasp_planning():\r\n    \"\"\"Demonstrate grasp planning\"\"\"\r\n    print(\"Demonstrating Grasp Planning for Robotics\")\r\n\r\n    # Initialize planner\r\n    planner = ManipulationPlanner()\r\n\r\n    # Example object info (from detection)\r\n    object_info = {\r\n        'dimensions_3d': np.array([0.08, 0.08, 0.15]),  # 8x8x15 cm object\r\n        'centroid_3d': np.array([0.5, 0.0, 0.1])      # Position in world coordinates\r\n    }\r\n\r\n    # Example object pose (from pose estimation)\r\n    object_pose = {\r\n        'position': np.array([0.5, 0.0, 0.1]),\r\n        'orientation': np.array([0, 0, 0, 1]),  # Identity quaternion\r\n        'object_type': 'cuboid_object',\r\n        'confidence': 0.9\r\n    }\r\n\r\n    # Target position for placing the object\r\n    target_position = np.array([0.8, 0.3, 0.1])\r\n\r\n    # Plan manipulation\r\n    result = planner.plan_manipulation(object_info, object_pose, target_position)\r\n\r\n    if result['success']:\r\n        print(f\"Manipulation plan generated successfully!\")\r\n        print(f\"Best grasp quality: {result['grasp_pose'].quality_score:.3f}\")\r\n        print(f\"Grasp type: {result['grasp_pose'].grasp_type.value}\")\r\n        print(f\"Sequence has {len(result['sequence'])} actions:\")\r\n\r\n        for i, action in enumerate(result['sequence']):\r\n            print(f\"  {i+1}. {action['description']}\")\r\n            if 'position' in action:\r\n                print(f\"     Position: [{action['position'][0]:.3f}, {action['position'][1]:.3f}, {action['position'][2]:.3f}]\")\r\n    else:\r\n        print(f\"Manipulation planning failed: {result['error']}\")\r\n\r\nif __name__ == \"__main__\":\r\n    demonstrate_grasp_planning()\n"})}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 33 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 32 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# python/ros2_integration.py\r\nimport rclpy\r\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\r\nfrom rclpy.node import Node\r\nfrom rclpy.executors import MultiThreadedExecutor\r\nfrom rclpy.qos import QoSProfile\r\nfrom rclpy.callback_groups import MutuallyExclusiveCallbackGroup\r\n\r\nfrom std_msgs.msg import String, Header\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import Pose, Point, Quaternion\r\nfrom builtin_interfaces.msg import Duration\r\n\r\n# Custom message types would be defined here\r\n# For this example, we\'ll define simplified versions\r\n\r\nclass ObjectDetectionActionServer(Node):\r\n    """ROS 2 action server for object detection"""\r\n\r\n    def __init__(self):\r\n        super().__init__(\'object_detection_action_server\')\r\n\r\n        # Initialize object detector\r\n        self.object_detector = YOLORobotDetector()  # From previous implementation\r\n        self.camera_info = None\r\n\r\n        # Setup action server\r\n        self._action_server = ActionServer(\r\n            self,\r\n            # Define custom action type\r\n            \'DetectObjects\',  # This would be your custom action\r\n            \'detect_objects\',\r\n            self.execute_callback,\r\n            goal_callback=self.goal_callback,\r\n            cancel_callback=self.cancel_callback\r\n        )\r\n\r\n        # Setup camera info subscriber\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera_info\',\r\n            self.camera_info_callback,\r\n            QoSProfile(depth=1)\r\n        )\r\n\r\n        # Setup image subscriber for continuous detection\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/image_raw\',\r\n            self.image_callback,\r\n            QoSProfile(depth=1)\r\n        )\r\n\r\n        self.get_logger().info("Object Detection Action Server initialized")\r\n\r\n    def goal_callback(self, goal_request):\r\n        """Accept or reject goal requests"""\r\n        self.get_logger().info(f"Received object detection goal")\r\n        return GoalResponse.ACCEPT\r\n\r\n    def cancel_callback(self, goal_handle):\r\n        """Accept or reject cancel requests"""\r\n        self.get_logger().info("Received cancel request for object detection")\r\n        return CancelResponse.ACCEPT\r\n\r\n    def camera_info_callback(self, msg):\r\n        """Update camera calibration info"""\r\n        self.camera_info = msg\r\n\r\n    def image_callback(self, msg):\r\n        """Process images for continuous detection"""\r\n        # Convert ROS Image to OpenCV\r\n        # This would convert the image and run detection\r\n        # For now, we\'ll just log the callback\r\n        pass\r\n\r\n    async def execute_callback(self, goal_handle):\r\n        """Execute object detection goal"""\r\n        self.get_logger().info("Executing object detection...")\r\n\r\n        feedback_msg = None  # Define feedback message\r\n        result = None  # Define result message\r\n\r\n        try:\r\n            # Process the detection request\r\n            # This would involve getting an image and running detection\r\n            image = self.get_latest_image()  # Implementation would get latest image\r\n            detections = self.object_detector.detect_objects(image)\r\n\r\n            # Format results\r\n            detection_results = []\r\n            for detection in detections:\r\n                detection_results.append({\r\n                    \'class_name\': detection.class_name,\r\n                    \'confidence\': detection.confidence,\r\n                    \'bbox\': detection.bbox,\r\n                    \'center_3d\': detection.center_3d\r\n                })\r\n\r\n            # Publish result\r\n            goal_handle.succeed()\r\n            # result = YourResultType(detections=detection_results)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in object detection: {e}")\r\n            goal_handle.abort()\r\n            # result = YourResultType(detections=[])\r\n\r\n        return result\r\n\r\n    def get_latest_image(self):\r\n        """Get the latest image from camera"""\r\n        # Implementation would return latest image\r\n        # For now, return a dummy image\r\n        return np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n\r\nclass ManipulationActionServer(Node):\r\n    """ROS 2 action server for manipulation planning and execution"""\r\n\r\n    def __init__(self):\r\n        super().__init__(\'manipulation_action_server\')\r\n\r\n        # Initialize manipulation planner\r\n        self.manipulation_planner = ManipulationPlanner()\r\n\r\n        # Setup action server\r\n        self._action_server = ActionServer(\r\n            self,\r\n            # Define custom action type\r\n            \'ExecuteManipulation\',  # This would be your custom action\r\n            \'execute_manipulation\',\r\n            self.execute_callback,\r\n            goal_callback=self.goal_callback,\r\n            cancel_callback=self.cancel_callback\r\n        )\r\n\r\n        # Publishers for robot control\r\n        self.joint_trajectory_pub = self.create_publisher(\r\n            # JointTrajectory,  # Replace with actual message type\r\n            \'joint_trajectory\',\r\n            QoSProfile(depth=1)\r\n        )\r\n\r\n        self.gripper_pub = self.create_publisher(\r\n            # GripperCommand,  # Replace with actual message type\r\n            \'gripper_command\',\r\n            QoSProfile(depth=1)\r\n        )\r\n\r\n        self.get_logger().info("Manipulation Action Server initialized")\r\n\r\n    def goal_callback(self, goal_request):\r\n        """Accept or reject manipulation goals"""\r\n        self.get_logger().info(f"Received manipulation goal: {goal_request.task_type}")\r\n        return GoalResponse.ACCEPT\r\n\r\n    def cancel_callback(self, goal_handle):\r\n        """Accept or reject cancel requests"""\r\n        self.get_logger().info("Received cancel request for manipulation")\r\n        return CancelResponse.ACCEPT\r\n\r\n    async def execute_callback(self, goal_handle):\r\n        """Execute manipulation goal"""\r\n        self.get_logger().info("Executing manipulation task...")\r\n\r\n        try:\r\n            # Plan manipulation based on goal\r\n            if goal_handle.request.task_type == \'grasp_object\':\r\n                result = await self._execute_grasp_task(goal_handle.request)\r\n            elif goal_handle.request.task_type == \'place_object\':\r\n                result = await self._execute_place_task(goal_handle.request)\r\n            elif goal_handle.request.task_type == \'transport_object\':\r\n                result = await self._execute_transport_task(goal_handle.request)\r\n            else:\r\n                result = await self._execute_custom_task(goal_handle.request)\r\n\r\n            goal_handle.succeed()\r\n            return result\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in manipulation: {e}")\r\n            goal_handle.abort()\r\n            # return YourResultType(success=False, error=str(e))\r\n\r\n    async def _execute_grasp_task(self, request):\r\n        """Execute grasp task"""\r\n        # Get object information from object detection\r\n        # Plan grasp\r\n        # Execute grasp sequence\r\n        pass\r\n\r\n    async def _execute_place_task(self, request):\r\n        """Execute place task"""\r\n        # Plan placement\r\n        # Execute placement sequence\r\n        pass\r\n\r\n    async def _execute_transport_task(self, request):\r\n        """Execute transport task"""\r\n        # Plan transport from source to destination\r\n        # Execute transport sequence\r\n        pass\r\n\r\n    async def _execute_custom_task(self, request):\r\n        """Execute custom manipulation task"""\r\n        # Handle custom manipulation tasks\r\n        pass\r\n\r\nclass ObjectManipulationClient(Node):\r\n    """Client for object detection and manipulation"""\r\n\r\n    def __init__(self):\r\n        super().__init__(\'object_manipulation_client\')\r\n\r\n        # Action clients\r\n        self.detection_client = rclpy.action.ActionClient(\r\n            self,\r\n            # Your detection action type\r\n            \'DetectObjects\',\r\n            \'detect_objects\'\r\n        )\r\n\r\n        self.manipulation_client = rclpy.action.ActionClient(\r\n            self,\r\n            # Your manipulation action type\r\n            \'ExecuteManipulation\',\r\n            \'execute_manipulation\'\r\n        )\r\n\r\n        # Publishers for commands\r\n        self.command_pub = self.create_publisher(String, \'manipulation_command\', 10)\r\n\r\n        self.get_logger().info("Object Manipulation Client initialized")\r\n\r\n    def detect_objects(self, timeout_sec: float = 10.0):\r\n        """Send object detection request"""\r\n        goal_msg = None  # Your goal message\r\n\r\n        if not self.detection_client.wait_for_server(timeout_sec=timeout_sec):\r\n            self.get_logger().error("Detection action server not available")\r\n            return None\r\n\r\n        send_goal_future = self.detection_client.send_goal_async(\r\n            goal_msg,\r\n            feedback_callback=self.detection_feedback_callback\r\n        )\r\n\r\n        rclpy.spin_until_future_complete(self, send_goal_future)\r\n        goal_handle = send_goal_future.result()\r\n\r\n        if not goal_handle.accepted:\r\n            self.get_logger().info("Detection goal rejected")\r\n            return None\r\n\r\n        get_result_future = goal_handle.get_result_async()\r\n        rclpy.spin_until_future_complete(self, get_result_future)\r\n\r\n        result = get_result_future.result().result\r\n        return result\r\n\r\n    def execute_manipulation(self, task_type: str, object_info: Dict, target_pose: Dict, timeout_sec: float = 30.0):\r\n        """Send manipulation execution request"""\r\n        goal_msg = None  # Your manipulation goal message\r\n\r\n        if not self.manipulation_client.wait_for_server(timeout_sec=timeout_sec):\r\n            self.get_logger().error("Manipulation action server not available")\r\n            return None\r\n\r\n        send_goal_future = self.manipulation_client.send_goal_async(\r\n            goal_msg,\r\n            feedback_callback=self.manipulation_feedback_callback\r\n        )\r\n\r\n        rclpy.spin_until_future_complete(self, send_goal_future)\r\n        goal_handle = send_goal_future.result()\r\n\r\n        if not goal_handle.accepted:\r\n            self.get_logger().info("Manipulation goal rejected")\r\n            return None\r\n\r\n        get_result_future = goal_handle.get_result_async()\r\n        rclpy.spin_until_future_complete(self, get_result_future)\r\n\r\n        result = get_result_future.result().result\r\n        return result\r\n\r\n    def detection_feedback_callback(self, feedback_msg):\r\n        """Handle detection feedback"""\r\n        self.get_logger().info(f"Detection feedback: {feedback_msg}")\r\n\r\n    def manipulation_feedback_callback(self, feedback_msg):\r\n        """Handle manipulation feedback"""\r\n        self.get_logger().info(f"Manipulation feedback: {feedback_msg}")\r\n\r\ndef main(args=None):\r\n    """Main function to run the object detection and manipulation nodes"""\r\n    rclpy.init(args=args)\r\n\r\n    # Create nodes\r\n    detection_server = ObjectDetectionActionServer()\r\n    manipulation_server = ManipulationActionServer()\r\n    client = ObjectManipulationClient()\r\n\r\n    # Use multi-threaded executor\r\n    executor = MultiThreadedExecutor(num_threads=4)\r\n    executor.add_node(detection_server)\r\n    executor.add_node(manipulation_server)\r\n    executor.add_node(client)\r\n\r\n    try:\r\n        executor.spin()\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        detection_server.destroy_node()\r\n        manipulation_server.destroy_node()\r\n        client.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 32 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 31 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# python/performance_optimization.py\r\nimport torch\r\nimport numpy as np\r\nimport cv2\r\nimport time\r\nfrom typing import List, Dict, Any, Callable\r\nimport threading\r\nfrom queue import Queue, Empty\r\nimport multiprocessing as mp\r\nfrom functools import wraps\r\nimport psutil\r\nimport os\r\n\r\ndef timing_decorator(func):\r\n    """Decorator to measure function execution time"""\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        start = time.time()\r\n        result = func(*args, **kwargs)\r\n        end = time.time()\r\n        print(f"{func.__name__} took {(end - start) * 1000:.2f}ms")\r\n        return result\r\n    return wrapper\r\n\r\nclass OptimizedObjectDetectionPipeline:\r\n    """Optimized pipeline for real-time object detection"""\r\n\r\n    def __init__(self, model_path: str = "yolov5s.pt", device: str = "cuda"):\r\n        self.device = device if torch.cuda.is_available() and device == "cuda" else "cpu"\r\n\r\n        # Load model with optimizations\r\n        self.model = torch.hub.load(\'ultralytics/yolov5\', \'custom\', path=model_path)\r\n        self.model.to(self.device)\r\n        self.model.eval()\r\n\r\n        # Apply optimizations\r\n        if self.device == "cuda":\r\n            self.model.half()  # Use half precision on GPU\r\n            torch.backends.cudnn.benchmark = True\r\n\r\n        # Pipeline components\r\n        self.input_queue = Queue(maxsize=2)  # Limit queue size to prevent memory buildup\r\n        self.output_queue = Queue(maxsize=2)\r\n        self.is_running = False\r\n        self.processing_thread = None\r\n\r\n        # Frame skipping for performance\r\n        self.frame_skip = 2  # Process every 2nd frame\r\n        self.frame_count = 0\r\n\r\n        # Statistics\r\n        self.stats = {\r\n            \'processed_frames\': 0,\r\n            \'average_fps\': 0.0,\r\n            \'average_detection_time\': 0.0,\r\n            \'memory_usage\': 0.0\r\n        }\r\n\r\n    def start_pipeline(self):\r\n        """Start the detection pipeline"""\r\n        self.is_running = True\r\n        self.processing_thread = threading.Thread(target=self._pipeline_loop, daemon=True)\r\n        self.processing_thread.start()\r\n\r\n    def stop_pipeline(self):\r\n        """Stop the detection pipeline"""\r\n        self.is_running = False\r\n        if self.processing_thread:\r\n            self.processing_thread.join()\r\n\r\n    def submit_frame(self, frame: np.ndarray) -> bool:\r\n        """Submit a frame for processing"""\r\n        try:\r\n            self.input_queue.put_nowait(frame)\r\n            return True\r\n        except:\r\n            # Queue is full, drop frame\r\n            return False\r\n\r\n    def get_results(self) -> List[DetectionResult]:\r\n        """Get detection results"""\r\n        try:\r\n            return self.output_queue.get_nowait()\r\n        except Empty:\r\n            return []\r\n\r\n    def _pipeline_loop(self):\r\n        """Main pipeline processing loop"""\r\n        while self.is_running:\r\n            try:\r\n                # Get frame from input queue\r\n                frame = self.input_queue.get(timeout=0.01)\r\n\r\n                # Apply frame skipping\r\n                self.frame_count += 1\r\n                if self.frame_count % self.frame_skip != 0:\r\n                    continue\r\n\r\n                # Process frame\r\n                start_time = time.time()\r\n                detections = self._process_frame_optimized(frame)\r\n                processing_time = time.time() - start_time\r\n\r\n                # Update statistics\r\n                self.stats[\'processed_frames\'] += 1\r\n                self.stats[\'average_detection_time\'] = (\r\n                    (self.stats[\'average_detection_time\'] * (self.stats[\'processed_frames\'] - 1) + processing_time) /\r\n                    self.stats[\'processed_frames\']\r\n                )\r\n\r\n                # Calculate FPS\r\n                if processing_time > 0:\r\n                    current_fps = 1.0 / processing_time\r\n                    if self.stats[\'average_fps\'] == 0:\r\n                        self.stats[\'average_fps\'] = current_fps\r\n                    else:\r\n                        # Exponential moving average\r\n                        self.stats[\'average_fps\'] = 0.9 * self.stats[\'average_fps\'] + 0.1 * current_fps\r\n\r\n                # Put results in output queue\r\n                try:\r\n                    self.output_queue.put_nowait(detections)\r\n                except:\r\n                    # Output queue full, results are lost\r\n                    pass\r\n\r\n            except Empty:\r\n                continue  # No frame available, continue loop\r\n            except Exception as e:\r\n                print(f"Error in pipeline: {e}")\r\n                continue\r\n\r\n    def _process_frame_optimized(self, frame: np.ndarray) -> List[DetectionResult]:\r\n        """Optimized frame processing"""\r\n        # Resize frame to model input size (smaller = faster)\r\n        h, w = frame.shape[:2]\r\n        target_size = 416  # Smaller than standard 640 for speed\r\n        scale = min(target_size / h, target_size / w)\r\n        new_h, new_w = int(h * scale), int(w * scale)\r\n\r\n        resized = cv2.resize(frame, (new_w, new_h))\r\n\r\n        # Pad to make it square\r\n        delta_w = target_size - new_w\r\n        delta_h = target_size - new_h\r\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\r\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\r\n\r\n        padded = cv2.copyMakeBorder(\r\n            resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[114, 114, 114]\r\n        )\r\n\r\n        # Convert to tensor and normalize\r\n        img_tensor = torch.from_numpy(padded).permute(2, 0, 1).float() / 255.0\r\n        img_tensor = img_tensor.unsqueeze(0).to(self.device)\r\n\r\n        # Run inference\r\n        with torch.no_grad():\r\n            results = self.model(img_tensor)\r\n\r\n        # Process results\r\n        detections = self._process_detections(results, frame.shape[:2])\r\n        return detections\r\n\r\n    def _process_detections(self, results, image_shape) -> List[DetectionResult]:\r\n        """Process YOLO detection results"""\r\n        detections = []\r\n        pred = results.pred[0]\r\n\r\n        if len(pred) > 0:\r\n            for *xyxy, conf, cls in pred.tolist():\r\n                x1, y1, x2, y2 = map(int, xyxy)\r\n                width = x2 - x1\r\n                height = y2 - y1\r\n\r\n                if conf >= 0.5:  # Confidence threshold\r\n                    detection = DetectionResult(\r\n                        class_id=int(cls),\r\n                        class_name=self.model.names[int(cls)],\r\n                        confidence=conf,\r\n                        bbox=(x1, y1, width, height),\r\n                        center_3d=None,\r\n                        rotation_3d=None\r\n                    )\r\n                    detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def get_performance_stats(self) -> Dict[str, Any]:\r\n        """Get performance statistics"""\r\n        process = psutil.Process(os.getpid())\r\n        self.stats[\'memory_usage\'] = process.memory_info().rss / 1024 / 1024  # MB\r\n        return self.stats.copy()\r\n\r\nclass MultiProcessDetection:\r\n    """Multi-process object detection for maximum performance"""\r\n\r\n    def __init__(self, num_processes: int = 2):\r\n        self.num_processes = num_processes\r\n        self.processes = []\r\n        self.input_queue = mp.Queue()\r\n        self.output_queue = mp.Queue()\r\n        self.is_running = False\r\n\r\n    def start_detection(self):\r\n        """Start multi-process detection"""\r\n        self.is_running = True\r\n\r\n        # Start worker processes\r\n        for i in range(self.num_processes):\r\n            p = mp.Process(target=self._worker_process, args=(i,))\r\n            p.start()\r\n            self.processes.append(p)\r\n\r\n    def stop_detection(self):\r\n        """Stop multi-process detection"""\r\n        self.is_running = False\r\n\r\n        # Send stop signals to all processes\r\n        for _ in range(self.num_processes):\r\n            self.input_queue.put(None)\r\n\r\n        # Wait for processes to finish\r\n        for p in self.processes:\r\n            p.join()\r\n\r\n    def _worker_process(self, worker_id: int):\r\n        """Worker process for detection"""\r\n        # Load model in each process\r\n        model = torch.hub.load(\'ultralytics/yolov5\', \'yolov5s\', pretrained=True)\r\n        model.eval()\r\n\r\n        while self.is_running:\r\n            try:\r\n                frame = self.input_queue.get(timeout=1.0)\r\n\r\n                if frame is None:  # Stop signal\r\n                    break\r\n\r\n                # Process frame\r\n                detections = self._process_frame(model, frame)\r\n\r\n                # Put results in output queue\r\n                self.output_queue.put((worker_id, detections))\r\n\r\n            except:\r\n                continue\r\n\r\n    def _process_frame(self, model, frame: np.ndarray):\r\n        """Process a single frame with the model"""\r\n        # Preprocess frame\r\n        img_tensor = self._preprocess_frame(frame)\r\n\r\n        # Run inference\r\n        with torch.no_grad():\r\n            results = model(img_tensor)\r\n\r\n        # Process results (simplified)\r\n        return results\r\n\r\n    def _preprocess_frame(self, frame: np.ndarray) -> torch.Tensor:\r\n        """Preprocess frame for model input"""\r\n        # Resize and normalize\r\n        resized = cv2.resize(frame, (640, 640))\r\n        img_tensor = torch.from_numpy(resized).permute(2, 0, 1).float() / 255.0\r\n        img_tensor = img_tensor.unsqueeze(0)\r\n        return img_tensor\r\n\r\nclass AdaptiveDetectionOptimizer:\r\n    """Adaptively optimize detection based on system load"""\r\n\r\n    def __init__(self):\r\n        self.current_fps = 30.0\r\n        self.target_fps = 30.0\r\n        self.confidence_threshold = 0.5\r\n        self.input_size = 416  # Starting input size\r\n        self.max_input_size = 640\r\n        self.min_input_size = 320\r\n\r\n    def adjust_parameters(self):\r\n        """Adjust detection parameters based on performance"""\r\n        # Monitor system resources\r\n        cpu_percent = psutil.cpu_percent()\r\n        memory_percent = psutil.virtual_memory().percent\r\n\r\n        # Adjust parameters based on load\r\n        if cpu_percent > 80 or memory_percent > 80:\r\n            # Reduce input size to improve performance\r\n            if self.input_size > self.min_input_size:\r\n                self.input_size -= 32\r\n                self.confidence_threshold += 0.05  # Increase threshold to reduce detections\r\n        elif cpu_percent < 50 and memory_percent < 60:\r\n            # Increase input size for better accuracy\r\n            if self.input_size < self.max_input_size:\r\n                self.input_size += 32\r\n                self.confidence_threshold -= 0.05  # Decrease threshold to allow more detections\r\n\r\n        # Adjust frame skip based on FPS\r\n        if self.current_fps < self.target_fps * 0.8:\r\n            # Too slow, increase frame skipping\r\n            pass  # Would implement frame skipping logic here\r\n        elif self.current_fps > self.target_fps * 1.2:\r\n            # Too fast, reduce frame skipping\r\n            pass\r\n\r\n    def get_current_settings(self) -> Dict[str, Any]:\r\n        """Get current optimization settings"""\r\n        return {\r\n            \'input_size\': self.input_size,\r\n            \'confidence_threshold\': self.confidence_threshold,\r\n            \'cpu_percent\': psutil.cpu_percent(),\r\n            \'memory_percent\': psutil.virtual_memory().percent\r\n        }\r\n\r\ndef benchmark_optimizations():\r\n    """Benchmark different optimization strategies"""\r\n    print("Benchmarking Object Detection Optimizations...")\r\n\r\n    # Create a sample image for testing\r\n    sample_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n\r\n    # Test different input sizes\r\n    input_sizes = [320, 416, 640]\r\n\r\n    print("\\nTesting different input sizes:")\r\n    for size in input_sizes:\r\n        start_time = time.time()\r\n\r\n        # Process multiple frames to get average time\r\n        for _ in range(10):\r\n            h, w = sample_image.shape[:2]\r\n            scale = min(size / h, size / w)\r\n            new_h, new_w = int(h * scale), int(w * scale)\r\n            resized = cv2.resize(sample_image, (new_w, new_h))\r\n\r\n            # Pad to make it square\r\n            delta_w = size - new_w\r\n            delta_h = size - new_h\r\n            top, bottom = delta_h // 2, delta_h - (delta_h // 2)\r\n            left, right = delta_w // 2, delta_w - (delta_w // 2)\r\n            padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[114, 114, 114])\r\n\r\n        elapsed = time.time() - start_time\r\n        avg_time = elapsed / 10 * 1000  # Convert to ms\r\n\r\n        print(f"  Input size {size}x{size}: {avg_time:.2f}ms per frame")\r\n\r\n    print("\\nOptimization benchmarking completed.")\r\n\r\ndef demonstrate_optimized_pipeline():\r\n    """Demonstrate optimized detection pipeline"""\r\n    print("Demonstrating Optimized Object Detection Pipeline")\r\n\r\n    try:\r\n        # Initialize optimized pipeline\r\n        pipeline = OptimizedObjectDetectionPipeline(model_path="yolov5s.pt")\r\n\r\n        # Start pipeline\r\n        pipeline.start_pipeline()\r\n\r\n        # Simulate frame submission\r\n        for i in range(100):  # Process 100 frames\r\n            sample_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n            pipeline.submit_frame(sample_frame)\r\n\r\n            # Get results occasionally\r\n            if i % 10 == 0:\r\n                results = pipeline.get_results()\r\n                stats = pipeline.get_performance_stats()\r\n                print(f"Frame {i}: FPS={stats[\'average_fps\']:.1f}, Detection time={stats[\'average_detection_time\']*1000:.1f}ms")\r\n\r\n        # Get final stats\r\n        final_stats = pipeline.get_performance_stats()\r\n        print(f"\\nFinal Performance Stats:")\r\n        print(f"  Average FPS: {final_stats[\'average_fps\']:.2f}")\r\n        print(f"  Average detection time: {final_stats[\'average_detection_time\']*1000:.2f}ms")\r\n        print(f"  Memory usage: {final_stats[\'memory_usage\']:.2f} MB")\r\n\r\n        # Stop pipeline\r\n        pipeline.stop_pipeline()\r\n\r\n    except Exception as e:\r\n        print(f"Error in optimized pipeline: {e}")\r\n        print("Make sure YOLOv5 is properly installed and model file exists")\r\n\r\nif __name__ == "__main__":\r\n    benchmark_optimizations()\r\n    demonstrate_optimized_pipeline()\n'})}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 31 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 30 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 30 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 29 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 29 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 29 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 28 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 28 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 27 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 27 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 26 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 26 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 25 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 25 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 25 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 24 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 24 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 23 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 23 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 22 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 22 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 22 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 21 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 21 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 20 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 20 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 19 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 19 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 18 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 18 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 17 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 17 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 17 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 16 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 16 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 15 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 15 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 15 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 15 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 15 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 14 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 14 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(n.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 13 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 13 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 04 MINUTES 12 SECONDS VISIT ",(0,i.jsx)(n.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]})]})}function T(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);