"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[220],{7221:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>T,contentTitle:()=>o,default:()=>E,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3/isaac-ros-vslam-perception","title":"isaac-ros-vslam-perception","description":"MYMEMORY WARNING//MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-3/isaac-ros-vslam-perception.md","sourceDirName":"module-3","slug":"/module-3/isaac-ros-vslam-perception","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ur/docs/module-3/isaac-ros-vslam-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Mrsaleem110/Physical-AI-humanoid-robotic-chatbot-book/tree/main/docs/docs/module-3/isaac-ros-vslam-perception.md","tags":[],"version":"current","lastUpdatedBy":"muhammad_saleem","lastUpdatedAt":1766408575000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"synthetic-data-generation","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ur/docs/module-3/synthetic-data-generation"},"next":{"title":"nav2-humanoid-locomotion","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ur/docs/module-3/nav2-humanoid-locomotion"}}');var i=n(4848),s=n(8453);const a={sidebar_position:2},o=void 0,T={},c=[];function A(e){const r={a:"a",code:"code",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 14 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 13 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 13 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 12 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 12 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 11 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 11 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 11 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 10 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 10 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 09 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 09 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 08 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 08 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 08 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 07 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 07 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 06 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 06 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 05 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 05 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 04 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 04 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 04 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# python/vslam_pipeline.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom nav_msgs.msg import Odometry\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom isaac_ros_seesaw import SeesawVSLAM\r\n\r\nclass IsaacVSLAMNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_vslam_node\')\r\n\r\n        # ROS 2 interfaces\r\n        self.bridge = CvBridge()\r\n        self.odom_publisher = self.create_publisher(Odometry, \'/visual_odom\', 10)\r\n        self.pose_publisher = self.create_publisher(PoseStamped, \'/vslam_pose\', 10)\r\n\r\n        # Subscribe to camera topics\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/rgb/camera_info\',\r\n            self.camera_info_callback,\r\n            10\r\n        )\r\n\r\n        # Initialize Isaac VSLAM\r\n        self.vslam = SeesawVSLAM(\r\n            max_features=2000,\r\n            min_distance=15,\r\n            matching_threshold=0.7\r\n        )\r\n\r\n        # State variables\r\n        self.camera_matrix = None\r\n        self.distortion_coeffs = None\r\n        self.previous_frame = None\r\n        self.current_pose = np.eye(4)\r\n\r\n        self.get_logger().info("Isaac VSLAM Node initialized")\r\n\r\n    def camera_info_callback(self, msg):\r\n        """Process camera calibration information"""\r\n        if self.camera_matrix is None:\r\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n            self.distortion_coeffs = np.array(msg.d)\r\n            self.get_logger().info(f"Camera calibration loaded: {self.camera_matrix.shape}")\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera images for VSLAM"""\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n\r\n            # Undistort image if camera parameters available\r\n            if self.camera_matrix is not None and self.distortion_coeffs is not None:\r\n                cv_image = cv2.undistort(\r\n                    cv_image,\r\n                    self.camera_matrix,\r\n                    self.distortion_coeffs,\r\n                    None,\r\n                    self.camera_matrix\r\n                )\r\n\r\n            # Run VSLAM\r\n            success, pose_increment = self.vslam.process_frame(cv_image)\r\n\r\n            if success:\r\n                # Update current pose\r\n                self.current_pose = self.current_pose @ pose_increment\r\n\r\n                # Publish odometry\r\n                self.publish_odometry(msg.header.stamp)\r\n\r\n                # Publish pose\r\n                self.publish_pose(msg.header.stamp)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing image: {e}")\r\n\r\n    def publish_odometry(self, timestamp):\r\n        """Publish visual odometry"""\r\n        odom_msg = Odometry()\r\n        odom_msg.header.stamp = timestamp\r\n        odom_msg.header.frame_id = "map"\r\n        odom_msg.child_frame_id = "base_link"\r\n\r\n        # Extract position and orientation from pose matrix\r\n        position = self.current_pose[:3, 3]\r\n        orientation = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\r\n\r\n        odom_msg.pose.pose.position.x = float(position[0])\r\n        odom_msg.pose.pose.position.y = float(position[1])\r\n        odom_msg.pose.pose.position.z = float(position[2])\r\n\r\n        odom_msg.pose.pose.orientation.x = float(orientation[0])\r\n        odom_msg.pose.pose.orientation.y = float(orientation[1])\r\n        odom_msg.pose.pose.orientation.z = float(orientation[2])\r\n        odom_msg.pose.pose.orientation.w = float(orientation[3])\r\n\r\n        self.odom_publisher.publish(odom_msg)\r\n\r\n    def publish_pose(self, timestamp):\r\n        """Publish pose estimate"""\r\n        pose_msg = PoseStamped()\r\n        pose_msg.header.stamp = timestamp\r\n        pose_msg.header.frame_id = "map"\r\n\r\n        position = self.current_pose[:3, 3]\r\n        orientation = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\r\n\r\n        pose_msg.pose.position.x = float(position[0])\r\n        pose_msg.pose.position.y = float(position[1])\r\n        pose_msg.pose.position.z = float(position[2])\r\n\r\n        pose_msg.pose.orientation.x = float(orientation[0])\r\n        pose_msg.pose.orientation.y = float(orientation[1])\r\n        pose_msg.pose.orientation.z = float(orientation[2])\r\n        pose_msg.pose.orientation.w = float(orientation[3])\r\n\r\n        self.pose_publisher.publish(pose_msg)\r\n\r\n    def rotation_matrix_to_quaternion(self, rotation_matrix):\r\n        """Convert rotation matrix to quaternion"""\r\n        # Method to convert rotation matrix to quaternion\r\n        trace = np.trace(rotation_matrix)\r\n\r\n        if trace > 0:\r\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\r\n            qw = 0.25 * s\r\n            qx = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\r\n            qy = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\r\n            qz = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\r\n        else:\r\n            if rotation_matrix[0, 0] > rotation_matrix[1, 1] and rotation_matrix[0, 0] > rotation_matrix[2, 2]:\r\n                s = np.sqrt(1.0 + rotation_matrix[0, 0] - rotation_matrix[1, 1] - rotation_matrix[2, 2]) * 2\r\n                qw = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\r\n                qx = 0.25 * s\r\n                qy = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\r\n                qz = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\r\n            elif rotation_matrix[1, 1] > rotation_matrix[2, 2]:\r\n                s = np.sqrt(1.0 + rotation_matrix[1, 1] - rotation_matrix[0, 0] - rotation_matrix[2, 2]) * 2\r\n                qw = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\r\n                qx = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\r\n                qy = 0.25 * s\r\n                qz = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\r\n            else:\r\n                s = np.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2\r\n                qw = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\r\n                qx = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\r\n                qy = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\r\n                qz = 0.25 * s\r\n\r\n        return [qx, qy, qz, qw]\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    vslam_node = IsaacVSLAMNode()\r\n\r\n    try:\r\n        rclpy.spin(vslam_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        vslam_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 03 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# python/accelerated_feature_detection.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import Header\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom isaac_ros.common import IsaacROSBase\r\nimport time\r\n\r\nclass AcceleratedFeatureDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'accelerated_feature_detection_node\')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Feature detector parameters\r\n        self.max_features = 2000\r\n        self.min_distance = 15\r\n        self.quality_level = 0.01\r\n        self.block_size = 3\r\n\r\n        # Initialize CUDA-based feature detector if available\r\n        self.use_cuda = self.check_cuda_support()\r\n        if self.use_cuda:\r\n            self.get_logger().info("CUDA-accelerated feature detection enabled")\r\n        else:\r\n            self.get_logger().info("Falling back to CPU-based feature detection")\r\n\r\n        self.get_logger().info("Accelerated feature detection node initialized")\r\n\r\n    def check_cuda_support(self):\r\n        """Check if CUDA is available for accelerated processing"""\r\n        try:\r\n            import pycuda.driver as cuda\r\n            import pycuda.autoinit\r\n            cuda.init()\r\n            return True\r\n        except ImportError:\r\n            self.get_logger().warn("PyCUDA not available, using CPU processing")\r\n            return False\r\n        except Exception as e:\r\n            self.get_logger().warn(f"CUDA not available: {e}")\r\n            return False\r\n\r\n    def image_callback(self, msg):\r\n        """Process image and detect features"""\r\n        try:\r\n            start_time = time.time()\r\n\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n\r\n            # Convert to grayscale if needed\r\n            if len(cv_image.shape) == 3:\r\n                gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\r\n            else:\r\n                gray = cv_image\r\n\r\n            # Detect features\r\n            if self.use_cuda:\r\n                keypoints = self.cuda_feature_detection(gray)\r\n            else:\r\n                keypoints = self.cpu_feature_detection(gray)\r\n\r\n            # Measure processing time\r\n            processing_time = time.time() - start_time\r\n\r\n            # Log performance\r\n            self.get_logger().debug(f"Feature detection: {len(keypoints)} features in {processing_time:.3f}s")\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in feature detection: {e}")\r\n\r\n    def cuda_feature_detection(self, gray_image):\r\n        """Perform feature detection using CUDA acceleration"""\r\n        # In practice, this would use NVIDIA\'s optimized CUDA kernels\r\n        # For now, we\'ll use a simplified approach that demonstrates the concept\r\n\r\n        # Detect Shi-Tomasi corners (good features to track)\r\n        corners = cv2.goodFeaturesToTrack(\r\n            gray_image,\r\n            maxCorners=self.max_features,\r\n            qualityLevel=self.quality_level,\r\n            minDistance=self.min_distance,\r\n            blockSize=self.block_size\r\n        )\r\n\r\n        if corners is not None:\r\n            keypoints = [cv2.KeyPoint(x=float(point[0][0]), y=float(point[0][1]), size=1) for point in corners]\r\n        else:\r\n            keypoints = []\r\n\r\n        return keypoints\r\n\r\n    def cpu_feature_detection(self, gray_image):\r\n        """Perform feature detection on CPU"""\r\n        # Detect Shi-Tomasi corners\r\n        corners = cv2.goodFeaturesToTrack(\r\n            gray_image,\r\n            maxCorners=self.max_features,\r\n            qualityLevel=self.quality_level,\r\n            minDistance=self.min_distance,\r\n            blockSize=self.block_size\r\n        )\r\n\r\n        if corners is not None:\r\n            keypoints = [cv2.KeyPoint(x=float(point[0][0]), y=float(point[0][1]), size=1) for point in corners]\r\n        else:\r\n            keypoints = []\r\n\r\n        return keypoints\r\n\r\n    def extract_descriptors(self, image, keypoints):\r\n        """Extract feature descriptors using SIFT or ORB"""\r\n        # Use ORB as it\'s faster and works well with GPU acceleration\r\n        orb = cv2.ORB_create(nfeatures=self.max_features)\r\n        keypoints, descriptors = orb.detectAndCompute(image, None)\r\n\r\n        return keypoints, descriptors\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    feature_node = AcceleratedFeatureDetectionNode()\r\n\r\n    try:\r\n        rclpy.spin(feature_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        feature_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 03 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 02 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# python/stereo_perception_pipeline.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom stereo_msgs.msg import DisparityImage\r\nfrom geometry_msgs.msg import PointStamped\r\nimport numpy as np\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass IsaacStereoPerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_stereo_perception_node\')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Stereo camera parameters\r\n        self.left_image = None\r\n        self.right_image = None\r\n        self.left_camera_info = None\r\n        self.right_camera_info = None\r\n        self.stereo_rectified = False\r\n\r\n        # ROS 2 subscriptions\r\n        self.left_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/left/image_raw\',\r\n            self.left_image_callback,\r\n            10\r\n        )\r\n\r\n        self.right_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/right/image_raw\',\r\n            self.right_image_callback,\r\n            10\r\n        )\r\n\r\n        self.left_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/left/camera_info\',\r\n            self.left_camera_info_callback,\r\n            10\r\n        )\r\n\r\n        self.right_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/right/camera_info\',\r\n            self.right_camera_info_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.disparity_pub = self.create_publisher(DisparityImage, \'/disparity\', 10)\r\n        self.point_cloud_pub = self.create_publisher(PointStamped, \'/point_cloud\', 10)\r\n\r\n        # Stereo processing parameters\r\n        self.num_disparities = 64\r\n        self.block_size = 11\r\n        self.min_disparity = 0\r\n\r\n        # Initialize stereo matcher\r\n        self.stereo = cv2.StereoSGBM_create(\r\n            minDisparity=self.min_disparity,\r\n            numDisparities=self.num_disparities,\r\n            blockSize=self.block_size,\r\n            P1=8 * 3 * self.block_size ** 2,\r\n            P2=32 * 3 * self.block_size ** 2,\r\n            disp12MaxDiff=1,\r\n            uniquenessRatio=15,\r\n            speckleWindowSize=0,\r\n            speckleRange=2,\r\n            preFilterCap=63,\r\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\r\n        )\r\n\r\n        self.get_logger().info("Isaac Stereo Perception Node initialized")\r\n\r\n    def left_image_callback(self, msg):\r\n        """Handle left camera image"""\r\n        self.left_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n        self.process_stereo_if_ready()\r\n\r\n    def right_image_callback(self, msg):\r\n        """Handle right camera image"""\r\n        self.right_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n        self.process_stereo_if_ready()\r\n\r\n    def left_camera_info_callback(self, msg):\r\n        """Handle left camera calibration info"""\r\n        self.left_camera_info = msg\r\n        self.compute_stereo_rectification()\r\n\r\n    def right_camera_info_callback(self, msg):\r\n        """Handle right camera calibration info"""\r\n        self.right_camera_info = msg\r\n        self.compute_stereo_rectification()\r\n\r\n    def compute_stereo_rectification(self):\r\n        """Compute stereo rectification parameters"""\r\n        if self.left_camera_info and self.right_camera_info:\r\n            # Extract camera matrices\r\n            left_cam_matrix = np.array(self.left_camera_info.k).reshape(3, 3)\r\n            right_cam_matrix = np.array(self.right_camera_info.k).reshape(3, 3)\r\n\r\n            # Extract distortion coefficients\r\n            left_dist_coeffs = np.array(self.left_camera_info.d)\r\n            right_dist_coeffs = np.array(self.right_camera_info.d)\r\n\r\n            # Extract rotation and translation between cameras\r\n            # This is simplified - in practice, you\'d have the extrinsic parameters\r\n            R = np.eye(3)  # Rotation matrix\r\n            T = np.array([-0.1, 0, 0])  # Translation vector (baseline)\r\n\r\n            # Compute rectification parameters\r\n            size = (self.left_camera_info.width, self.left_camera_info.height)\r\n\r\n            R1, R2, P1, P2, Q, validPixROI1, validPixROI2 = cv2.stereoRectify(\r\n                left_cam_matrix, left_dist_coeffs,\r\n                right_cam_matrix, right_dist_coeffs,\r\n                size, R, T,\r\n                flags=cv2.CALIB_ZERO_DISPARITY,\r\n                alpha=-1  # Full rectification\r\n            )\r\n\r\n            # Initialize rectification maps\r\n            self.left_map1, self.left_map2 = cv2.initUndistortRectifyMap(\r\n                left_cam_matrix, left_dist_coeffs, R1, P1, size, cv2.CV_32FC1\r\n            )\r\n\r\n            self.right_map1, self.right_map2 = cv2.initUndistortRectifyMap(\r\n                right_cam_matrix, right_dist_coeffs, R2, P2, size, cv2.CV_32FC1\r\n            )\r\n\r\n            self.stereo_rectified = True\r\n            self.get_logger().info("Stereo rectification computed successfully")\r\n\r\n    def process_stereo_if_ready(self):\r\n        """Process stereo images if both images and calibration are available"""\r\n        if (self.left_image is not None and\r\n            self.right_image is not None and\r\n            self.stereo_rectified):\r\n\r\n            # Rectify images\r\n            left_rectified = cv2.remap(\r\n                self.left_image, self.left_map1, self.left_map2,\r\n                interpolation=cv2.INTER_LINEAR\r\n            )\r\n\r\n            right_rectified = cv2.remap(\r\n                self.right_image, self.right_map1, self.right_map2,\r\n                interpolation=cv2.INTER_LINEAR\r\n            )\r\n\r\n            # Convert to grayscale\r\n            left_gray = cv2.cvtColor(left_rectified, cv2.COLOR_BGR2GRAY)\r\n            right_gray = cv2.cvtColor(right_rectified, cv2.COLOR_BGR2GRAY)\r\n\r\n            # Compute disparity\r\n            disparity = self.stereo.compute(left_gray, right_gray).astype(np.float32) / 16.0\r\n\r\n            # Publish disparity\r\n            self.publish_disparity(disparity)\r\n\r\n    def publish_disparity(self, disparity):\r\n        """Publish disparity image"""\r\n        # Create disparity message\r\n        disparity_msg = DisparityImage()\r\n        disparity_msg.header.stamp = self.get_clock().now().to_msg()\r\n        disparity_msg.header.frame_id = "camera_link"\r\n\r\n        # Set disparity image data\r\n        disparity_msg.image = self.bridge.cv2_to_imgmsg(disparity, encoding="32FC1")\r\n        disparity_msg.f = 616.3  # Focal length (example value)\r\n        disparity_msg.T = 0.1     # Baseline (example value)\r\n        disparity_msg.min_disparity = 0.0\r\n        disparity_msg.max_disparity = 64.0\r\n        disparity_msg.delta_d = 0.125\r\n\r\n        self.disparity_pub.publish(disparity_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    stereo_node = IsaacStereoPerceptionNode()\r\n\r\n    try:\r\n        rclpy.spin(stereo_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        stereo_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 02 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"# python/object_detection_pipeline.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\r\nfrom geometry_msgs.msg import Point\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nimport torch\r\nfrom torchvision import transforms\r\nfrom isaac_ros.detection import IsaacObjectDetector\r\n\r\nclass IsaacObjectDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_object_detection_node')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to camera image\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publish detections\r\n        self.detection_pub = self.create_publisher(Detection2DArray, '/detections', 10)\r\n\r\n        # Initialize Isaac object detector\r\n        self.detector = IsaacObjectDetector(\r\n            model_name='detr_resnet50',\r\n            confidence_threshold=0.5,\r\n            max_detections=20\r\n        )\r\n\r\n        # Class names for COCO dataset (common in Isaac ROS)\r\n        self.class_names = [\r\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\r\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\r\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\r\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\r\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\r\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\r\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\r\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\r\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\r\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\r\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\r\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\r\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\r\n        ]\r\n\r\n        self.get_logger().info(\"Isaac Object Detection Node initialized\")\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process image and detect objects\"\"\"\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\r\n\r\n            # Run object detection\r\n            detections = self.detector.detect(cv_image)\r\n\r\n            # Create detection message\r\n            detection_array_msg = Detection2DArray()\r\n            detection_array_msg.header = msg.header\r\n\r\n            # Process detections\r\n            for detection in detections:\r\n                detection_msg = Detection2D()\r\n\r\n                # Set header\r\n                detection_msg.header = msg.header\r\n\r\n                # Set bounding box\r\n                bbox = detection['bbox']\r\n                detection_msg.bbox.size_x = int(bbox['width'])\r\n                detection_msg.bbox.size_y = int(bbox['height'])\r\n\r\n                # Set center point\r\n                center_x = int(bbox['x'] + bbox['width'] / 2)\r\n                center_y = int(bbox['y'] + bbox['height'] / 2)\r\n                detection_msg.bbox.center.x = center_x\r\n                detection_msg.bbox.center.y = center_y\r\n                detection_msg.bbox.center.z = 0.0  # 2D detection\r\n\r\n                # Set results\r\n                hypothesis = ObjectHypothesisWithPose()\r\n                class_id = detection['class_id']\r\n                confidence = detection['confidence']\r\n\r\n                hypothesis.id = str(class_id)\r\n                hypothesis.score = confidence\r\n\r\n                detection_msg.results.append(hypothesis)\r\n\r\n                # Add class name if available\r\n                if class_id < len(self.class_names):\r\n                    class_name_hypothesis = ObjectHypothesisWithPose()\r\n                    class_name_hypothesis.id = self.class_names[class_id]\r\n                    class_name_hypothesis.score = confidence\r\n                    detection_msg.results.append(class_name_hypothesis)\r\n\r\n                detection_array_msg.detections.append(detection_msg)\r\n\r\n            # Publish detections\r\n            self.detection_pub.publish(detection_array_msg)\r\n\r\n            # Log detection results\r\n            self.get_logger().debug(f\"Published {len(detections)} detections\")\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\"Error in object detection: {e}\")\r\n\r\nclass IsaacObjectDetector:\r\n    \"\"\"Wrapper for Isaac ROS object detection\"\"\"\r\n    def __init__(self, model_name='detr_resnet50', confidence_threshold=0.5, max_detections=20):\r\n        self.model_name = model_name\r\n        self.confidence_threshold = confidence_threshold\r\n        self.max_detections = max_detections\r\n\r\n        # Initialize detection model\r\n        self.model = self.load_model(model_name)\r\n\r\n    def load_model(self, model_name):\r\n        \"\"\"Load the specified detection model\"\"\"\r\n        # In practice, this would load the Isaac ROS detection model\r\n        # For now, we'll return a placeholder\r\n        self.get_logger().info(f\"Loading model: {model_name}\")\r\n        return model_name\r\n\r\n    def detect(self, image):\r\n        \"\"\"Run object detection on image\"\"\"\r\n        # This would interface with Isaac ROS detection packages\r\n        # For demonstration, return dummy detections\r\n        detections = []\r\n\r\n        # Simulate detections (in real implementation, this would use the actual model)\r\n        # Here we'd typically use Isaac ROS DETR or other detection nodes\r\n        if self.model_name == 'detr_resnet50':\r\n            # Simulate detection results\r\n            # In practice, this would process the image with the actual model\r\n            pass\r\n\r\n        # Return mock detections for demonstration\r\n        mock_detections = [\r\n            {\r\n                'bbox': {'x': 100, 'y': 100, 'width': 200, 'height': 200},\r\n                'class_id': 0,  # person\r\n                'confidence': 0.85\r\n            },\r\n            {\r\n                'bbox': {'x': 400, 'y': 200, 'width': 150, 'height': 150},\r\n                'class_id': 56,  # chair\r\n                'confidence': 0.78\r\n            }\r\n        ]\r\n\r\n        # Filter based on confidence threshold\r\n        detections = [\r\n            det for det in mock_detections\r\n            if det['confidence'] >= self.confidence_threshold\r\n        ]\r\n\r\n        # Limit to max detections\r\n        detections = detections[:self.max_detections]\r\n\r\n        return detections\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    detection_node = IsaacObjectDetectionNode()\r\n\r\n    try:\r\n        rclpy.spin(detection_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        detection_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 01 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 01 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# python/cuda_perception.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import Float32\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport time\r\nfrom numba import cuda\r\nimport math\r\n\r\nclass CudaPerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'cuda_perception_node\')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # ROS interfaces\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.performance_pub = self.create_publisher(Float32, \'/cuda_performance\', 10)\r\n\r\n        # Check CUDA availability\r\n        self.cuda_available = cuda.is_available()\r\n        if self.cuda_available:\r\n            self.get_logger().info("CUDA acceleration available")\r\n            self.gpu_device = cuda.get_current_device()\r\n            self.get_logger().info(f"Using GPU: {self.gpu_device.name}")\r\n        else:\r\n            self.get_logger().warn("CUDA not available, falling back to CPU")\r\n\r\n        self.get_logger().info("CUDA Perception Node initialized")\r\n\r\n    @cuda.jit\r\n    def cuda_color_conversion_kernel(self, input_image, output_image):\r\n        """CUDA kernel for RGB to grayscale conversion"""\r\n        x, y = cuda.grid(2)\r\n        rows, cols = input_image.shape[0], input_image.shape[1]\r\n\r\n        if x < rows and y < cols:\r\n            # Convert RGB to grayscale using luminance formula\r\n            r = input_image[x, y, 0]\r\n            g = input_image[x, y, 1]\r\n            b = input_image[x, y, 2]\r\n\r\n            gray_value = 0.299 * r + 0.587 * g + 0.114 * b\r\n            output_image[x, y] = gray_value\r\n\r\n    @cuda.jit\r\n    def cuda_edge_detection_kernel(self, input_image, output_image):\r\n        """CUDA kernel for simple edge detection"""\r\n        x, y = cuda.grid(2)\r\n        rows, cols = input_image.shape[0], input_image.shape[1]\r\n\r\n        if x > 0 and x < rows - 1 and y > 0 and y < cols - 1:\r\n            # Simple Sobel edge detection\r\n            gx = (input_image[x-1, y-1] + 2*input_image[x, y-1] + input_image[x+1, y-1] -\r\n                  input_image[x-1, y+1] - 2*input_image[x, y+1] - input_image[x+1, y+1])\r\n\r\n            gy = (input_image[x-1, y-1] + 2*input_image[x-1, y] + input_image[x-1, y+1] -\r\n                  input_image[x+1, y-1] - 2*input_image[x+1, y] - input_image[x+1, y+1])\r\n\r\n            magnitude = math.sqrt(gx*gx + gy*gy)\r\n            output_image[x, y] = min(255.0, magnitude)\r\n\r\n    def image_callback(self, msg):\r\n        """Process image using CUDA acceleration"""\r\n        try:\r\n            start_time = time.time()\r\n\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n\r\n            if not self.cuda_available:\r\n                # Fallback to CPU processing\r\n                self.cpu_processing(cv_image)\r\n                return\r\n\r\n            # Transfer image to GPU\r\n            gpu_image = cuda.to_device(cv_image)\r\n\r\n            # Prepare output arrays\r\n            gray_image = np.zeros((cv_image.shape[0], cv_image.shape[1]), dtype=np.float32)\r\n            gpu_gray = cuda.to_device(gray_image)\r\n\r\n            # Configure CUDA grid\r\n            threads_per_block = (16, 16)\r\n            blocks_per_grid_x = math.ceil(cv_image.shape[0] / threads_per_block[0])\r\n            blocks_per_grid_y = math.ceil(cv_image.shape[1] / threads_per_block[1])\r\n            blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\r\n\r\n            # Run color conversion kernel\r\n            self.cuda_color_conversion_kernel[blocks_per_grid, threads_per_block](\r\n                gpu_image, gpu_gray\r\n            )\r\n\r\n            # Copy result back to host\r\n            result_gray = gpu_gray.copy_to_host()\r\n\r\n            # Perform edge detection on grayscale image\r\n            edge_image = np.zeros_like(result_gray)\r\n            gpu_edge = cuda.to_device(edge_image)\r\n\r\n            self.cuda_edge_detection_kernel[blocks_per_grid, threads_per_block](\r\n                gpu_gray, gpu_edge\r\n            )\r\n\r\n            result_edge = gpu_edge.copy_to_host()\r\n\r\n            # Measure performance\r\n            processing_time = time.time() - start_time\r\n\r\n            # Publish performance metric\r\n            perf_msg = Float32()\r\n            perf_msg.data = 1.0 / processing_time if processing_time > 0 else 0.0\r\n            self.performance_pub.publish(perf_msg)\r\n\r\n            self.get_logger().debug(f"CUDA processing time: {processing_time:.3f}s, FPS: {perf_msg.data:.1f}")\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in CUDA processing: {e}")\r\n\r\n    def cpu_processing(self, image):\r\n        """CPU fallback for image processing"""\r\n        start_time = time.time()\r\n\r\n        # Convert to grayscale using CPU\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n        # Apply edge detection\r\n        edges = cv2.Canny(gray, 50, 150)\r\n\r\n        processing_time = time.time() - start_time\r\n\r\n        # Publish performance metric\r\n        perf_msg = Float32()\r\n        perf_msg.data = 1.0 / processing_time if processing_time > 0 else 0.0\r\n        self.performance_pub.publish(perf_msg)\r\n\r\n        self.get_logger().debug(f"CPU processing time: {processing_time:.3f}s, FPS: {perf_msg.data:.1f}")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    cuda_node = CudaPerceptionNode()\r\n\r\n    try:\r\n        rclpy.spin(cuda_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        cuda_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 00 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# python/nitros_pipeline.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nfrom isaac_ros.nitros import NitrosType, NitrosPublisher, NitrosSubscriber\r\n\r\nclass NitrosPerceptionPipelineNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'nitros_perception_pipeline_node\')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Initialize NITROS types for optimized data transport\r\n        self.nitros_image_type = NitrosType(\r\n            name=\'nitros_image_bgr8\',\r\n            supported_types=[\'sensor_msgs/msg/Image\']\r\n        )\r\n\r\n        self.nitros_camera_info_type = NitrosType(\r\n            name=\'nitros_camera_info\',\r\n            supported_types=[\'sensor_msgs/msg/CameraInfo\']\r\n        )\r\n\r\n        # Create NITROS subscribers\r\n        self.image_sub = NitrosSubscriber(\r\n            self,\r\n            self.nitros_image_type,\r\n            \'camera/rgb/image_raw\',\r\n            qos_profile=10\r\n        )\r\n\r\n        self.camera_info_sub = NitrosSubscriber(\r\n            self,\r\n            self.nitros_camera_info_type,\r\n            \'camera/rgb/camera_info\',\r\n            qos_profile=10\r\n        )\r\n\r\n        # Create NITROS publisher\r\n        self.processed_image_pub = NitrosPublisher(\r\n            self,\r\n            self.nitros_image_type,\r\n            \'camera/rgb/processed_image\',\r\n            qos_profile=10\r\n        )\r\n\r\n        # Synchronize topics using NITROS\r\n        self.image_sub.registerCallback(self.image_callback)\r\n        self.camera_info_sub.registerCallback(self.camera_info_callback)\r\n\r\n        # Processing state\r\n        self.camera_matrix = None\r\n        self.latest_image = None\r\n\r\n        self.get_logger().info("NITROS Perception Pipeline initialized")\r\n\r\n    def image_callback(self, msg):\r\n        """Handle incoming image message"""\r\n        try:\r\n            # Convert to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n\r\n            # Store latest image for processing\r\n            self.latest_image = cv_image\r\n\r\n            # Process image if camera calibration is available\r\n            if self.camera_matrix is not None:\r\n                processed_image = self.process_image_with_calibration(cv_image)\r\n            else:\r\n                processed_image = self.process_image(cv_image)\r\n\r\n            # Publish processed image\r\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding=\'bgr8\')\r\n            processed_msg.header = msg.header\r\n            self.processed_image_pub.publish(processed_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing image: {e}")\r\n\r\n    def camera_info_callback(self, msg):\r\n        """Handle camera calibration information"""\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.distortion_coeffs = np.array(msg.d)\r\n\r\n        self.get_logger().info(f"Camera calibration updated: {self.camera_matrix.shape}")\r\n\r\n    def process_image_with_calibration(self, image):\r\n        """Process image with camera calibration"""\r\n        # Undistort image using calibration parameters\r\n        undistorted = cv2.undistort(\r\n            image,\r\n            self.camera_matrix,\r\n            self.distortion_coeffs,\r\n            None,\r\n            self.camera_matrix\r\n        )\r\n\r\n        # Apply some processing (e.g., enhance features)\r\n        processed = self.enhance_features(undistorted)\r\n\r\n        return processed\r\n\r\n    def process_image(self, image):\r\n        """Process image without calibration"""\r\n        # Apply basic processing\r\n        processed = self.enhance_features(image)\r\n\r\n        return processed\r\n\r\n    def enhance_features(self, image):\r\n        """Enhance image features for better perception"""\r\n        # Convert to LAB color space for better feature enhancement\r\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\r\n\r\n        # Enhance the L channel\r\n        l, a, b = cv2.split(lab)\r\n\r\n        # Apply CLAHE to L channel\r\n        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\r\n        l = clahe.apply(l)\r\n\r\n        # Merge channels back\r\n        enhanced_lab = cv2.merge([l, a, b])\r\n\r\n        # Convert back to BGR\r\n        enhanced = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\r\n\r\n        return enhanced\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    nitros_node = NitrosPerceptionPipelineNode()\r\n\r\n    try:\r\n        rclpy.spin(nitros_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        nitros_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 00 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 08 MINUTES 00 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# python/perception_action_coupling.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, JointState\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass PerceptionActionCouplingNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'perception_action_coupling_node\')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Robot state\r\n        self.joint_states = None\r\n        self.robot_pose = None\r\n\r\n        # Perception data\r\n        self.latest_image = None\r\n        self.object_detections = []\r\n\r\n        # ROS interfaces\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.joint_state_sub = self.create_subscription(\r\n            JointState,\r\n            \'/joint_states\',\r\n            self.joint_state_callback,\r\n            10\r\n        )\r\n\r\n        self.pose_sub = self.create_subscription(\r\n            PoseStamped,\r\n            \'/vslam_pose\',\r\n            self.pose_callback,\r\n            10\r\n        )\r\n\r\n        # Robot control publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.joint_cmd_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\r\n\r\n        # State tracking\r\n        self.robot_mode = "exploring"  # exploring, tracking, manipulating\r\n        self.target_object = None\r\n\r\n        self.get_logger().info("Perception-Action Coupling Node initialized")\r\n\r\n    def image_callback(self, msg):\r\n        """Process image and detect objects for action planning"""\r\n        try:\r\n            # Convert to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n            self.latest_image = cv_image\r\n\r\n            # Run object detection (simplified)\r\n            detections = self.detect_objects(cv_image)\r\n            self.object_detections = detections\r\n\r\n            # Update target based on detections\r\n            self.update_target(detections)\r\n\r\n            # Plan actions based on perception\r\n            self.plan_actions()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in image processing: {e}")\r\n\r\n    def detect_objects(self, image):\r\n        """Detect objects in image"""\r\n        # Simplified object detection\r\n        # In practice, this would use Isaac ROS detection nodes\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n        # Detect circles as simple objects\r\n        circles = cv2.HoughCircles(\r\n            gray,\r\n            cv2.HOUGH_GRADIENT,\r\n            dp=1,\r\n            minDist=50,\r\n            param1=50,\r\n            param2=30,\r\n            minRadius=10,\r\n            maxRadius=100\r\n        )\r\n\r\n        detections = []\r\n        if circles is not None:\r\n            circles = np.round(circles[0, :]).astype("int")\r\n            for (x, y, r) in circles:\r\n                detections.append({\r\n                    \'center\': (x, y),\r\n                    \'radius\': r,\r\n                    \'bbox\': (x-r, y-r, 2*r, 2*r),\r\n                    \'class\': \'circle_object\',\r\n                    \'confidence\': 0.8\r\n                })\r\n\r\n        return detections\r\n\r\n    def joint_state_callback(self, msg):\r\n        """Update robot joint states"""\r\n        self.joint_states = msg\r\n\r\n    def pose_callback(self, msg):\r\n        """Update robot pose from VSLAM"""\r\n        self.robot_pose = msg\r\n\r\n    def update_target(self, detections):\r\n        """Update target object based on detections"""\r\n        if detections:\r\n            # For simplicity, target the first detected object\r\n            # In practice, this would use more sophisticated targeting logic\r\n            self.target_object = detections[0]\r\n\r\n            # Determine action based on object type\r\n            if detections[0][\'class\'] == \'circle_object\':\r\n                self.robot_mode = "tracking"\r\n\r\n    def plan_actions(self):\r\n        """Plan robot actions based on perception"""\r\n        if self.target_object is not None and self.robot_pose is not None:\r\n            # Calculate relative position to target\r\n            image_center = (320, 240)  # Assuming 640x480 image\r\n            target_center = self.target_object[\'center\']\r\n\r\n            # Calculate angular offset\r\n            dx = target_center[0] - image_center[0]\r\n            dy = target_center[1] - image_center[1]\r\n\r\n            # Normalize to [-1, 1] range\r\n            norm_dx = dx / (image_center[0])  # 320 is half image width\r\n            norm_dy = dy / (image_center[1])  # 240 is half image height\r\n\r\n            # Convert to robot commands\r\n            cmd = Twist()\r\n\r\n            # Proportional control for turning toward target\r\n            cmd.angular.z = -norm_dx * 0.5  # Negative for correct direction\r\n\r\n            # Forward movement if target is centered\r\n            if abs(norm_dx) < 0.1:  # If roughly centered\r\n                cmd.linear.x = min(0.2, 0.1 * (1 - abs(norm_dy)))  # Move forward if not too close\r\n\r\n            # Publish command\r\n            self.cmd_vel_pub.publish(cmd)\r\n\r\n    def execute_manipulation(self):\r\n        """Execute manipulation action if target is close enough"""\r\n        if self.target_object is not None:\r\n            # Check if target is within reach\r\n            # This would involve more complex 3D position estimation\r\n            # and inverse kinematics\r\n\r\n            # For demonstration, we\'ll simulate joint commands\r\n            if self.joint_states is not None:\r\n                cmd_msg = JointState()\r\n                cmd_msg.header.stamp = self.get_clock().now().to_msg()\r\n                cmd_msg.name = self.joint_states.name  # Use same joint names\r\n                cmd_msg.position = list(self.joint_states.position)  # Start with current positions\r\n\r\n                # Modify joint positions for reaching\r\n                # This is simplified - real implementation would use inverse kinematics\r\n                if \'right_shoulder_joint\' in cmd_msg.name:\r\n                    idx = cmd_msg.name.index(\'right_shoulder_joint\')\r\n                    cmd_msg.position[idx] += 0.1  # Move shoulder up\r\n\r\n                if \'right_elbow_joint\' in cmd_msg.name:\r\n                    idx = cmd_msg.name.index(\'right_elbow_joint\')\r\n                    cmd_msg.position[idx] += 0.1  # Bend elbow\r\n\r\n                self.joint_cmd_pub.publish(cmd_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    coupling_node = PerceptionActionCouplingNode()\r\n\r\n    try:\r\n        rclpy.spin(coupling_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        coupling_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 59 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 59 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"# python/performance_benchmarking.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import Float32, Int32\r\nfrom sensor_msgs.msg import Image\r\nimport time\r\nimport numpy as np\r\nfrom collections import deque\r\n\r\nclass IsaacPerfBenchmarkNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_perf_benchmark_node')\r\n\r\n        # Performance tracking\r\n        self.fps_history = deque(maxlen=100)\r\n        self.processing_times = deque(maxlen=100)\r\n        self.memory_usage = deque(maxlen=100)\r\n\r\n        # Publishers\r\n        self.fps_pub = self.create_publisher(Float32, '/performance/fps', 10)\r\n        self.processing_time_pub = self.create_publisher(Float32, '/performance/processing_time', 10)\r\n        self.load_pub = self.create_publisher(Float32, '/performance/load', 10)\r\n\r\n        # Subscriber for performance monitoring\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Timer for publishing metrics\r\n        self.timer = self.create_timer(1.0, self.publish_performance_metrics)\r\n\r\n        # Performance counters\r\n        self.frame_count = 0\r\n        self.last_time = time.time()\r\n\r\n        self.get_logger().info(\"Isaac Performance Benchmark Node initialized\")\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process image and track performance\"\"\"\r\n        start_time = time.time()\r\n\r\n        # Simulate image processing time\r\n        # In real implementation, this would be the actual processing\r\n        time.sleep(0.01)  # Simulate 10ms processing time\r\n\r\n        processing_time = time.time() - start_time\r\n        self.processing_times.append(processing_time)\r\n\r\n        # Update frame count\r\n        self.frame_count += 1\r\n\r\n        # Calculate FPS\r\n        current_time = time.time()\r\n        if current_time - self.last_time >= 1.0:\r\n            fps = self.frame_count / (current_time - self.last_time)\r\n            self.fps_history.append(fps)\r\n\r\n            self.frame_count = 0\r\n            self.last_time = current_time\r\n\r\n    def publish_performance_metrics(self):\r\n        \"\"\"Publish performance metrics\"\"\"\r\n        if self.fps_history:\r\n            avg_fps = np.mean(self.fps_history)\r\n            avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0.0\r\n\r\n            # Calculate load (0.0 to 1.0) - higher FPS is better, lower processing time is better\r\n            max_target_fps = 30.0  # Target FPS\r\n            load = min(1.0, avg_processing_time * max_target_fps)\r\n\r\n            # Publish metrics\r\n            fps_msg = Float32()\r\n            fps_msg.data = float(avg_fps)\r\n            self.fps_pub.publish(fps_msg)\r\n\r\n            proc_time_msg = Float32()\r\n            proc_time_msg.data = float(avg_processing_time * 1000)  # Convert to milliseconds\r\n            self.processing_time_pub.publish(proc_time_msg)\r\n\r\n            load_msg = Float32()\r\n            load_msg.data = float(load)\r\n            self.load_pub.publish(load_msg)\r\n\r\n            # Log performance\r\n            self.get_logger().info(\r\n                f\"Performance: {avg_fps:.1f} FPS, \"\r\n                f\"{avg_processing_time*1000:.1f}ms, \"\r\n                f\"Load: {load:.2f}\"\r\n            )\r\n\r\nclass IsaacPerfAnalyzer:\r\n    \"\"\"Performance analyzer for Isaac ROS pipelines\"\"\"\r\n    def __init__(self):\r\n        self.metrics = {\r\n            'latency': [],\r\n            'throughput': [],\r\n            'memory_usage': [],\r\n            'cpu_usage': [],\r\n            'gpu_usage': []\r\n        }\r\n\r\n    def measure_latency(self, input_time, output_time):\r\n        \"\"\"Measure processing latency\"\"\"\r\n        latency = output_time - input_time\r\n        self.metrics['latency'].append(latency)\r\n        return latency\r\n\r\n    def measure_throughput(self, num_operations, time_interval):\r\n        \"\"\"Measure processing throughput\"\"\"\r\n        throughput = num_operations / time_interval\r\n        self.metrics['throughput'].append(throughput)\r\n        return throughput\r\n\r\n    def analyze_performance(self):\r\n        \"\"\"Analyze performance metrics\"\"\"\r\n        analysis = {}\r\n\r\n        if self.metrics['latency']:\r\n            analysis['avg_latency'] = np.mean(self.metrics['latency'])\r\n            analysis['max_latency'] = np.max(self.metrics['latency'])\r\n            analysis['p95_latency'] = np.percentile(self.metrics['latency'], 95)\r\n\r\n        if self.metrics['throughput']:\r\n            analysis['avg_throughput'] = np.mean(self.metrics['throughput'])\r\n            analysis['min_throughput'] = np.min(self.metrics['throughput'])\r\n\r\n        return analysis\r\n\r\n    def get_optimization_recommendations(self):\r\n        \"\"\"Get recommendations for performance optimization\"\"\"\r\n        analysis = self.analyze_performance()\r\n        recommendations = []\r\n\r\n        if analysis.get('avg_latency', 0) > 0.1:  # 100ms threshold\r\n            recommendations.append(\"High latency detected - consider reducing processing complexity or increasing hardware resources\")\r\n\r\n        if analysis.get('avg_throughput', 0) < 10:  # 10 ops/sec threshold\r\n            recommendations.append(\"Low throughput - consider optimizing algorithms or using more efficient data structures\")\r\n\r\n        return recommendations\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    perf_node = IsaacPerfBenchmarkNode()\r\n\r\n    try:\r\n        rclpy.spin(perf_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        perf_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 58 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 57 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 57 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 57 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 56 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 56 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 55 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 55 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 54 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 54 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 53 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 53 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 53 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 52 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 52 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 51 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 51 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 50 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 50 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 50 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 49 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 49 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 48 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 48 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 47 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 47 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 46 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 46 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 46 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 45 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 45 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 44 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 44 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 44 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 44 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 43 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]}),"\n",(0,i.jsxs)(r.p,{children:["MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 43 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 42 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE\r\nMYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  20 HOURS 07 MINUTES 42 SECONDS VISIT ",(0,i.jsx)(r.a,{href:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP",children:"HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP"})," TO TRANSLATE MORE"]})]})}function E(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(A,{...e})}):A(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>a,x:()=>o});var t=n(6540);const i={},s=t.createContext(i);function a(e){const r=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:r},e.children)}}}]);