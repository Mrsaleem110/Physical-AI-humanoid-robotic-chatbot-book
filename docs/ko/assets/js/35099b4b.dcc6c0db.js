"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[173],{5035:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-2/sensor-simulation","title":"Sensor Simulation: LiDAR, Depth Camera, IMU","description":"Chapter Objectives","source":"@site/docs/module-2/sensor-simulation.md","sourceDirName":"module-2","slug":"/module-2/sensor-simulation","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ko/docs/module-2/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Mrsaleem110/Physical-AI-humanoid-robotic-chatbot-book/tree/main/docs/docs/module-2/sensor-simulation.md","tags":[],"version":"current","lastUpdatedBy":"muhammad_saleem","lastUpdatedAt":1765979454000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Unity-Based HRI Visualization","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ko/docs/module-2/unity-hri-visualization"},"next":{"title":"Isaac Sim Photorealistic Simulation","permalink":"/Physical-AI-humanoid-robotic-chatbot-book/ko/docs/module-3/isaac-sim-photorealistic"}}');var a=r(4848),t=r(8453);const o={sidebar_position:4},s="Sensor Simulation: LiDAR, Depth Camera, IMU",l={},d=[{value:"Chapter Objectives",id:"chapter-objectives",level:2},{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"Sensor Categories for Humanoid Robots",id:"sensor-categories-for-humanoid-robots",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"LiDAR Physics and Characteristics",id:"lidar-physics-and-characteristics",level:3},{value:"LiDAR Simulation in Gazebo",id:"lidar-simulation-in-gazebo",level:3},{value:"LiDAR Simulation in Unity",id:"lidar-simulation-in-unity",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Depth Camera Characteristics",id:"depth-camera-characteristics",level:3},{value:"Depth Camera Simulation in Unity",id:"depth-camera-simulation-in-unity",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Characteristics",id:"imu-characteristics",level:3},{value:"IMU Simulation Implementation",id:"imu-simulation-implementation",level:3},{value:"Sensor Fusion and Integration",id:"sensor-fusion-and-integration",level:2},{value:"Multi-Sensor Data Integration",id:"multi-sensor-data-integration",level:3},{value:"Best Practices for Sensor Simulation",id:"best-practices-for-sensor-simulation",level:2},{value:"Realistic Noise Modeling",id:"realistic-noise-modeling",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Validation and Testing",id:"validation-and-testing",level:3},{value:"Integration with ROS/ROS 2",id:"integration-with-rosros-2",level:2},{value:"ROS Message Publishing",id:"ros-message-publishing",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Summary",id:"summary",level:2},{value:"Learning Path Adjustment",id:"learning-path-adjustment",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"sensor-simulation-lidar-depth-camera-imu",children:"Sensor Simulation: LiDAR, Depth Camera, IMU"})}),"\n",(0,a.jsx)(n.h2,{id:"chapter-objectives",children:"Chapter Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement realistic LiDAR sensor simulation"}),"\n",(0,a.jsx)(n.li,{children:"Create depth camera simulation with realistic noise models"}),"\n",(0,a.jsx)(n.li,{children:"Simulate IMU sensors with drift and noise characteristics"}),"\n",(0,a.jsx)(n.li,{children:"Integrate sensor data with ROS/ROS 2 for robotics applications"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation is crucial for humanoid robotics development as it:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Provides realistic sensor data for algorithm development"}),"\n",(0,a.jsx)(n.li,{children:"Enables testing without physical sensors"}),"\n",(0,a.jsx)(n.li,{children:"Allows simulation of sensor failures and edge cases"}),"\n",(0,a.jsx)(n.li,{children:"Supports AI training with synthetic data"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-categories-for-humanoid-robots",children:"Sensor Categories for Humanoid Robots"}),"\n",(0,a.jsx)(n.p,{children:"Humanoid robots typically use several sensor types:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LiDAR"}),": 360-degree distance measurements for mapping and navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Cameras"}),": 3D point clouds for object recognition and manipulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IMU"}),": Inertial measurements for orientation and motion"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Other Sensors"}),": GPS, force/torque, tactile, etc."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-physics-and-characteristics",children:"LiDAR Physics and Characteristics"}),"\n",(0,a.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) sensors emit laser pulses and measure the time of flight to determine distances. Key characteristics include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range"}),": Maximum distance measurement (typically 5-100m)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Angular resolution (typically 0.1\xb0-1\xb0)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Field of View"}),": Horizontal and vertical coverage"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Rate"}),": How frequently measurements are taken"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Accuracy"}),": Measurement precision and noise characteristics"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lidar-simulation-in-gazebo",children:"LiDAR Simulation in Gazebo"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- URDF snippet for LiDAR sensor --\x3e\r\n<gazebo reference="lidar_link">\r\n  <sensor name="lidar" type="ray">\r\n    <pose>0 0 0 0 0 0</pose>\r\n    <visualize>true</visualize>\r\n    <update_rate>10</update_rate>\r\n    <ray>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>360</samples>\r\n          <resolution>1</resolution>\r\n          <min_angle>-3.14159</min_angle>\r\n          <max_angle>3.14159</max_angle>\r\n        </horizontal>\r\n      </scan>\r\n      <range>\r\n        <min>0.1</min>\r\n        <max>30.0</max>\r\n        <resolution>0.01</resolution>\r\n      </range>\r\n    </ray>\r\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\r\n      <ros>\r\n        <namespace>lidar</namespace>\r\n        <remapping>~/out:=scan</remapping>\r\n      </ros>\r\n      <output_type>sensor_msgs/LaserScan</output_type>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"lidar-simulation-in-unity",children:"LiDAR Simulation in Unity"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// Scripts/LiDARSimulator.cs\r\nusing UnityEngine;\r\nusing System.Collections.Generic;\r\n\r\npublic class LiDARSimulator : MonoBehaviour\r\n{\r\n    [Header("LiDAR Configuration")]\r\n    public int beamCount = 360;\r\n    public float minAngle = -Mathf.PI;\r\n    public float maxAngle = Mathf.PI;\r\n    public float maxRange = 30f;\r\n    public float updateRate = 10f; // Hz\r\n    public LayerMask detectionLayers = -1;\r\n\r\n    [Header("Noise Parameters")]\r\n    public float noiseStdDev = 0.01f;\r\n    public float biasError = 0f;\r\n\r\n    [Header("Visualization")]\r\n    public bool visualizeBeams = true;\r\n    public LineRenderer lineRenderer;\r\n\r\n    private float[] ranges;\r\n    private float updateInterval;\r\n    private float lastUpdate;\r\n\r\n    void Start()\r\n    {\r\n        ranges = new float[beamCount];\r\n        updateInterval = 1f / updateRate;\r\n        lastUpdate = -updateInterval; // Allow immediate first update\r\n\r\n        if (lineRenderer == null)\r\n        {\r\n            lineRenderer = GetComponent<LineRenderer>();\r\n            if (lineRenderer == null)\r\n            {\r\n                lineRenderer = gameObject.AddComponent<LineRenderer>();\r\n                lineRenderer.material = new Material(Shader.Find("Sprites/Default"));\r\n                lineRenderer.startWidth = 0.01f;\r\n                lineRenderer.endWidth = 0.01f;\r\n                lineRenderer.positionCount = beamCount;\r\n            }\r\n        }\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        if (Time.time - lastUpdate >= updateInterval)\r\n        {\r\n            SimulateLiDAR();\r\n            lastUpdate = Time.time;\r\n        }\r\n    }\r\n\r\n    void SimulateLiDAR()\r\n    {\r\n        for (int i = 0; i < beamCount; i++)\r\n        {\r\n            float angle = Mathf.Lerp(minAngle, maxAngle, (float)i / (beamCount - 1));\r\n            Vector3 direction = new Vector3(Mathf.Cos(angle), 0, Mathf.Sin(angle));\r\n\r\n            // Perform raycast\r\n            RaycastHit hit;\r\n            if (Physics.Raycast(transform.position, transform.TransformDirection(direction), out hit, maxRange, detectionLayers))\r\n            {\r\n                float distance = hit.distance;\r\n\r\n                // Add noise\r\n                float noisyDistance = AddNoise(distance);\r\n\r\n                ranges[i] = noisyDistance;\r\n            }\r\n            else\r\n            {\r\n                ranges[i] = float.MaxValue; // No hit\r\n            }\r\n        }\r\n\r\n        if (visualizeBeams)\r\n        {\r\n            UpdateVisualization();\r\n        }\r\n\r\n        // Publish simulated data (in a real implementation, this would publish to ROS)\r\n        PublishLiDARData();\r\n    }\r\n\r\n    float AddNoise(float trueValue)\r\n    {\r\n        // Add Gaussian noise\r\n        float noise = RandomGaussian() * noiseStdDev;\r\n        return trueValue + noise + biasError;\r\n    }\r\n\r\n    float RandomGaussian()\r\n    {\r\n        // Box-Muller transform for Gaussian random numbers\r\n        float u1 = Random.value;\r\n        float u2 = Random.value;\r\n        return Mathf.Sqrt(-2f * Mathf.Log(u1)) * Mathf.Cos(2f * Mathf.PI * u2);\r\n    }\r\n\r\n    void UpdateVisualization()\r\n    {\r\n        Vector3[] positions = new Vector3[beamCount];\r\n        for (int i = 0; i < beamCount; i++)\r\n        {\r\n            float angle = Mathf.Lerp(minAngle, maxAngle, (float)i / (beamCount - 1));\r\n            Vector3 direction = new Vector3(Mathf.Cos(angle), 0, Mathf.Sin(angle));\r\n\r\n            if (ranges[i] < maxRange)\r\n            {\r\n                positions[i] = transform.position + transform.TransformDirection(direction) * ranges[i];\r\n            }\r\n            else\r\n            {\r\n                positions[i] = transform.position + transform.TransformDirection(direction) * maxRange * 0.9f;\r\n            }\r\n        }\r\n\r\n        lineRenderer.SetPositions(positions);\r\n    }\r\n\r\n    void PublishLiDARData()\r\n    {\r\n        // In a real implementation, this would publish to ROS/ROS 2\r\n        // For now, we\'ll just log the data\r\n        Debug.Log($"LiDAR: {beamCount} beams, first range: {ranges[0]:F3}m");\r\n    }\r\n\r\n    public float[] GetRanges()\r\n    {\r\n        return ranges;\r\n    }\r\n\r\n    public float GetRange(int beamIndex)\r\n    {\r\n        if (beamIndex >= 0 && beamIndex < ranges.Length)\r\n        {\r\n            return ranges[beamIndex];\r\n        }\r\n        return float.MaxValue;\r\n    }\r\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-characteristics",children:"Depth Camera Characteristics"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras provide 3D point cloud data by measuring distance to objects in the scene. Key characteristics include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Image dimensions (e.g., 640x480, 1280x720)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Field of View"}),": Horizontal and vertical angles"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Range"}),": Minimum and maximum measurable distances"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Accuracy"}),": Distance measurement precision"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Frame Rate"}),": How often images are captured"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-simulation-in-unity",children:"Depth Camera Simulation in Unity"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// Scripts/DepthCameraSimulator.cs\r\nusing UnityEngine;\r\nusing System.Collections;\r\n\r\npublic class DepthCameraSimulator : MonoBehaviour\r\n{\r\n    [Header("Camera Configuration")]\r\n    public int width = 640;\r\n    public int height = 480;\r\n    public float fieldOfView = 60f;\r\n    public float nearClip = 0.1f;\r\n    public float farClip = 10f;\r\n\r\n    [Header("Noise Parameters")]\r\n    public float noiseStdDev = 0.02f;\r\n    public float biasError = 0f;\r\n    public float depthScale = 1f;\r\n\r\n    [Header("Output Settings")]\r\n    public bool outputPointCloud = true;\r\n    public bool outputDepthImage = true;\r\n\r\n    private Camera depthCamera;\r\n    private RenderTexture depthTexture;\r\n    private float[,] depthData;\r\n    private GameObject pointCloudObject;\r\n\r\n    void Start()\r\n    {\r\n        SetupCamera();\r\n        CreateDepthTexture();\r\n        depthData = new float[width, height];\r\n    }\r\n\r\n    void SetupCamera()\r\n    {\r\n        depthCamera = GetComponent<Camera>();\r\n        if (depthCamera == null)\r\n        {\r\n            depthCamera = gameObject.AddComponent<Camera>();\r\n        }\r\n\r\n        depthCamera.fieldOfView = fieldOfView;\r\n        depthCamera.nearClipPlane = nearClip;\r\n        depthCamera.farClipPlane = farClip;\r\n        depthCamera.depth = -1; // Render after other cameras\r\n        depthCamera.clearFlags = CameraClearFlags.SolidColor;\r\n        depthCamera.backgroundColor = Color.white;\r\n        depthCamera.enabled = false; // We\'ll render manually\r\n    }\r\n\r\n    void CreateDepthTexture()\r\n    {\r\n        depthTexture = new RenderTexture(width, height, 24, RenderTextureFormat.Depth);\r\n        depthTexture.Create();\r\n        depthCamera.targetTexture = depthTexture;\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        SimulateDepthCamera();\r\n    }\r\n\r\n    void SimulateDepthCamera()\r\n    {\r\n        // Render the scene from this camera\'s perspective\r\n        depthCamera.Render();\r\n\r\n        // Read depth data from render texture\r\n        ReadDepthData();\r\n\r\n        // Add noise to simulate real sensor characteristics\r\n        AddNoiseToDepthData();\r\n\r\n        if (outputPointCloud)\r\n        {\r\n            GeneratePointCloud();\r\n        }\r\n\r\n        if (outputDepthImage)\r\n        {\r\n            OutputDepthImage();\r\n        }\r\n    }\r\n\r\n    void ReadDepthData()\r\n    {\r\n        // Create a temporary render texture to read the depth data\r\n        RenderTexture currentRT = RenderTexture.active;\r\n        RenderTexture.active = depthTexture;\r\n\r\n        Texture2D depthTex = new Texture2D(width, height, TextureFormat.RFloat, false);\r\n        depthTex.ReadPixels(new Rect(0, 0, width, height), 0, 0);\r\n        depthTex.Apply();\r\n\r\n        // Convert texture data to depth values\r\n        Color[] pixels = depthTex.GetPixels();\r\n        for (int y = 0; y < height; y++)\r\n        {\r\n            for (int x = 0; x < width; x++)\r\n            {\r\n                int index = y * width + x;\r\n                float rawDepth = pixels[index].r;\r\n\r\n                // Convert raw depth to actual distance\r\n                // This is a simplified conversion - real implementation would be more complex\r\n                float actualDepth = ConvertRawDepthToDistance(rawDepth);\r\n                depthData[x, y] = actualDepth;\r\n            }\r\n        }\r\n\r\n        // Clean up\r\n        RenderTexture.active = currentRT;\r\n        DestroyImmediate(depthTex);\r\n    }\r\n\r\n    float ConvertRawDepthToDistance(float rawDepth)\r\n    {\r\n        // Convert raw depth buffer value to actual distance\r\n        // This is a simplified conversion\r\n        float zNear = depthCamera.nearClipPlane;\r\n        float zFar = depthCamera.farClipPlane;\r\n\r\n        // Convert from [0,1] to actual distance\r\n        float linearDepth = (2.0f * zNear * zFar) / (zFar + zNear - rawDepth * (zFar - zNear));\r\n        return linearDepth;\r\n    }\r\n\r\n    void AddNoiseToDepthData()\r\n    {\r\n        for (int y = 0; y < height; y++)\r\n        {\r\n            for (int x = 0; x < width; x++)\r\n            {\r\n                if (depthData[x, y] < farClip) // Only add noise to valid measurements\r\n                {\r\n                    float noise = RandomGaussian() * noiseStdDev;\r\n                    depthData[x, y] += noise + biasError;\r\n\r\n                    // Ensure depth is within valid range\r\n                    depthData[x, y] = Mathf.Clamp(depthData[x, y], nearClip, farClip);\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    float RandomGaussian()\r\n    {\r\n        // Box-Muller transform for Gaussian random numbers\r\n        float u1 = Random.value;\r\n        float u2 = Random.value;\r\n        return Mathf.Sqrt(-2f * Mathf.Log(u1)) * Mathf.Cos(2f * Mathf.PI * u2);\r\n    }\r\n\r\n    void GeneratePointCloud()\r\n    {\r\n        // Generate point cloud from depth data\r\n        Vector3[] points = new Vector3[width * height];\r\n        int pointCount = 0;\r\n\r\n        float fovX = fieldOfView * Mathf.Deg2Rad;\r\n        float fovY = (fieldOfView * height / width) * Mathf.Deg2Rad;\r\n\r\n        for (int y = 0; y < height; y++)\r\n        {\r\n            for (int x = 0; x < width; x++)\r\n            {\r\n                float depth = depthData[x, y];\r\n\r\n                if (depth > 0 && depth < farClip) // Valid depth measurement\r\n                {\r\n                    // Convert pixel coordinates to 3D coordinates\r\n                    float u = (float)x / width - 0.5f; // -0.5 to 0.5\r\n                    float v = (float)y / height - 0.5f; // -0.5 to 0.5\r\n\r\n                    float x3d = u * depth * Mathf.Tan(fovX / 2) * 2;\r\n                    float y3d = -v * depth * Mathf.Tan(fovY / 2) * 2; // Negative because screen coordinates are inverted\r\n                    float z3d = depth;\r\n\r\n                    Vector3 point = transform.TransformPoint(new Vector3(x3d, y3d, z3d));\r\n                    points[pointCount] = point;\r\n                    pointCount++;\r\n                }\r\n            }\r\n        }\r\n\r\n        // Resize array to actual point count\r\n        System.Array.Resize(ref points, pointCount);\r\n\r\n        // In a real implementation, you would publish this point cloud to ROS\r\n        Debug.Log($"Generated point cloud with {pointCount} points");\r\n    }\r\n\r\n    void OutputDepthImage()\r\n    {\r\n        // Create a texture to visualize the depth data\r\n        Texture2D depthVisualization = new Texture2D(width, height, TextureFormat.RGB24, false);\r\n\r\n        for (int y = 0; y < height; y++)\r\n        {\r\n            for (int x = 0; x < width; x++)\r\n            {\r\n                float depth = depthData[x, y];\r\n\r\n                // Normalize depth for visualization (0 = near, 1 = far)\r\n                float normalizedDepth = Mathf.InverseLerp(nearClip, farClip, depth);\r\n\r\n                // Map to grayscale color\r\n                Color color = Color.Lerp(Color.white, Color.black, normalizedDepth);\r\n                depthVisualization.SetPixel(x, y, color);\r\n            }\r\n        }\r\n\r\n        depthVisualization.Apply();\r\n\r\n        // In a real implementation, you would publish this image to ROS\r\n    }\r\n\r\n    public float[,] GetDepthData()\r\n    {\r\n        return depthData;\r\n    }\r\n\r\n    public float GetDepthAt(int x, int y)\r\n    {\r\n        if (x >= 0 && x < width && y >= 0 && y < height)\r\n        {\r\n            return depthData[x, y];\r\n        }\r\n        return float.MaxValue;\r\n    }\r\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"imu-characteristics",children:"IMU Characteristics"}),"\n",(0,a.jsx)(n.p,{children:"An IMU (Inertial Measurement Unit) typically combines:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Magnetometer"}),": Measures magnetic field (for heading)"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Key characteristics include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bias"}),": Systematic offset in measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise"}),": Random fluctuations in measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Drift"}),": Slow accumulation of errors over time"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scale Factor Error"}),": Mismatch between input and output"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cross-Axis Sensitivity"}),": Sensitivity to inputs in other axes"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"imu-simulation-implementation",children:"IMU Simulation Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// Scripts/IMUSimulator.cs\r\nusing UnityEngine;\r\n\r\n[System.Serializable]\r\npublic struct IMUReading\r\n{\r\n    public Vector3 linearAcceleration;  // m/s^2\r\n    public Vector3 angularVelocity;     // rad/s\r\n    public Vector3 magneticField;       // Tesla\r\n    public Vector3 orientation;         // Euler angles in radians\r\n    public double timestamp;            // Simulation time\r\n}\r\n\r\npublic class IMUSimulator : MonoBehaviour\r\n{\r\n    [Header("Accelerometer Parameters")]\r\n    public float accelerometerNoiseDensity = 8.75e-4f; // m/s^2 / sqrt(Hz)\r\n    public float accelerometerRandomWalk = 2.92e-5f;   // m/s^3 / sqrt(Hz)\r\n    public Vector3 accelerometerBias = Vector3.zero;\r\n    public float accelerometerScaleFactorError = 0.001f; // 0.1%\r\n\r\n    [Header("Gyroscope Parameters")]\r\n    public float gyroscopeNoiseDensity = 1.64e-4f; // rad/s / sqrt(Hz)\r\n    public float gyroscopeRandomWalk = 5.42e-6f;   // rad/s^2 / sqrt(Hz)\r\n    public Vector3 gyroscopeBias = Vector3.zero;\r\n    public float gyroscopeScaleFactorError = 0.001f; // 0.1%\r\n\r\n    [Header("Magnetometer Parameters")]\r\n    public float magnetometerNoise = 0.05e-6f; // Tesla\r\n    public Vector3 magnetometerBias = Vector3.zero;\r\n\r\n    [Header("Simulation Parameters")]\r\n    public float updateRate = 100f; // Hz\r\n    public bool enableDrift = true;\r\n\r\n    private IMUReading currentReading;\r\n    private float updateInterval;\r\n    private float lastUpdate;\r\n\r\n    // Bias drift accumulators\r\n    private Vector3 accelerometerBiasDrift = Vector3.zero;\r\n    private Vector3 gyroscopeBiasDrift = Vector3.zero;\r\n\r\n    void Start()\r\n    {\r\n        updateInterval = 1f / updateRate;\r\n        lastUpdate = -updateInterval; // Allow immediate first update\r\n\r\n        // Initialize with current transform values\r\n        UpdateIMUReading();\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        if (Time.time - lastUpdate >= updateInterval)\r\n        {\r\n            UpdateIMUReading();\r\n            lastUpdate = Time.time;\r\n        }\r\n    }\r\n\r\n    void UpdateIMUReading()\r\n    {\r\n        // Get true values from Unity\'s physics\r\n        Vector3 trueLinearAcceleration = GetTrueLinearAcceleration();\r\n        Vector3 trueAngularVelocity = GetTrueAngularVelocity();\r\n        Vector3 trueMagneticField = GetTrueMagneticField();\r\n        Vector3 trueOrientation = transform.eulerAngles * Mathf.Deg2Rad;\r\n\r\n        // Add noise and errors\r\n        currentReading.linearAcceleration = AddAccelerometerNoise(trueLinearAcceleration);\r\n        currentReading.angularVelocity = AddGyroscopeNoise(trueAngularVelocity);\r\n        currentReading.magneticField = AddMagnetometerNoise(trueMagneticField);\r\n        currentReading.orientation = trueOrientation; // Add noise to orientation if needed\r\n        currentReading.timestamp = Time.time;\r\n\r\n        // Publish simulated data\r\n        PublishIMUData();\r\n    }\r\n\r\n    Vector3 GetTrueLinearAcceleration()\r\n    {\r\n        // Calculate true linear acceleration from Unity physics\r\n        // This is a simplified approach - in practice, you\'d need to account for gravity\r\n        Rigidbody rb = GetComponent<Rigidbody>();\r\n        if (rb != null)\r\n        {\r\n            // Remove gravity from the acceleration\r\n            return rb.velocity - Physics.gravity * Time.deltaTime;\r\n        }\r\n        else\r\n        {\r\n            // If no rigidbody, use transform changes\r\n            return (transform.position - transform.position) / (Time.deltaTime * Time.deltaTime);\r\n        }\r\n    }\r\n\r\n    Vector3 GetTrueAngularVelocity()\r\n    {\r\n        // Calculate true angular velocity from Unity physics\r\n        Rigidbody rb = GetComponent<Rigidbody>();\r\n        if (rb != null)\r\n        {\r\n            return rb.angularVelocity;\r\n        }\r\n        else\r\n        {\r\n            // If no rigidbody, approximate from rotation change\r\n            return Vector3.zero; // Simplified - would need to track rotation over time\r\n        }\r\n    }\r\n\r\n    Vector3 GetTrueMagneticField()\r\n    {\r\n        // Return a typical Earth magnetic field vector\r\n        // This should be adjusted based on geographic location\r\n        return new Vector3(0.22f, 0.0f, 0.45f) * 1e-6f; // Tesla\r\n    }\r\n\r\n    Vector3 AddAccelerometerNoise(Vector3 trueAcceleration)\r\n    {\r\n        Vector3 noise = new Vector3(\r\n            RandomGaussian() * accelerometerNoiseDensity,\r\n            RandomGaussian() * accelerometerNoiseDensity,\r\n            RandomGaussian() * accelerometerNoiseDensity\r\n        );\r\n\r\n        // Add bias (with drift if enabled)\r\n        Vector3 bias = accelerometerBias + accelerometerBiasDrift;\r\n\r\n        // Add scale factor error\r\n        Vector3 scaledAcceleration = new Vector3(\r\n            trueAcceleration.x * (1 + accelerometerScaleFactorError),\r\n            trueAcceleration.y * (1 + accelerometerScaleFactorError),\r\n            trueAcceleration.z * (1 + accelerometerScaleFactorError)\r\n        );\r\n\r\n        // Add random walk to bias\r\n        if (enableDrift)\r\n        {\r\n            accelerometerBiasDrift += new Vector3(\r\n                RandomGaussian() * accelerometerRandomWalk * Mathf.Sqrt(updateInterval),\r\n                RandomGaussian() * accelerometerRandomWalk * Mathf.Sqrt(updateInterval),\r\n                RandomGaussian() * accelerometerRandomWalk * Mathf.Sqrt(updateInterval)\r\n            );\r\n        }\r\n\r\n        return scaledAcceleration + noise + bias;\r\n    }\r\n\r\n    Vector3 AddGyroscopeNoise(Vector3 trueAngularVelocity)\r\n    {\r\n        Vector3 noise = new Vector3(\r\n            RandomGaussian() * gyroscopeNoiseDensity,\r\n            RandomGaussian() * gyroscopeNoiseDensity,\r\n            RandomGaussian() * gyroscopeNoiseDensity\r\n        );\r\n\r\n        // Add bias (with drift if enabled)\r\n        Vector3 bias = gyroscopeBias + gyroscopeBiasDrift;\r\n\r\n        // Add scale factor error\r\n        Vector3 scaledAngularVelocity = new Vector3(\r\n            trueAngularVelocity.x * (1 + gyroscopeScaleFactorError),\r\n            trueAngularVelocity.y * (1 + gyroscopeScaleFactorError),\r\n            trueAngularVelocity.z * (1 + gyroscopeScaleFactorError)\r\n        );\r\n\r\n        // Add random walk to bias\r\n        if (enableDrift)\r\n        {\r\n            gyroscopeBiasDrift += new Vector3(\r\n                RandomGaussian() * gyroscopeRandomWalk * Mathf.Sqrt(updateInterval),\r\n                RandomGaussian() * gyroscopeRandomWalk * Mathf.Sqrt(updateInterval),\r\n                RandomGaussian() * gyroscopeRandomWalk * Mathf.Sqrt(updateInterval)\r\n            );\r\n        }\r\n\r\n        return scaledAngularVelocity + noise + bias;\r\n    }\r\n\r\n    Vector3 AddMagnetometerNoise(Vector3 trueMagneticField)\r\n    {\r\n        Vector3 noise = new Vector3(\r\n            RandomGaussian() * magnetometerNoise,\r\n            RandomGaussian() * magnetometerNoise,\r\n            RandomGaussian() * magnetometerNoise\r\n        );\r\n\r\n        // Add bias\r\n        Vector3 bias = magnetometerBias;\r\n\r\n        return trueMagneticField + noise + bias;\r\n    }\r\n\r\n    float RandomGaussian()\r\n    {\r\n        // Box-Muller transform for Gaussian random numbers\r\n        float u1 = Random.value;\r\n        float u2 = Random.value;\r\n        return Mathf.Sqrt(-2f * Mathf.Log(u1)) * Mathf.Cos(2f * Mathf.PI * u2);\r\n    }\r\n\r\n    void PublishIMUData()\r\n    {\r\n        // In a real implementation, this would publish to ROS/ROS 2\r\n        Debug.Log($"IMU: Acc=({currentReading.linearAcceleration.x:F3}, {currentReading.linearAcceleration.y:F3}, {currentReading.linearAcceleration.z:F3})");\r\n    }\r\n\r\n    public IMUReading GetIMUReading()\r\n    {\r\n        return currentReading;\r\n    }\r\n\r\n    public void ResetBiasDrift()\r\n    {\r\n        accelerometerBiasDrift = Vector3.zero;\r\n        gyroscopeBiasDrift = Vector3.zero;\r\n    }\r\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-and-integration",children:"Sensor Fusion and Integration"}),"\n",(0,a.jsx)(n.h3,{id:"multi-sensor-data-integration",children:"Multi-Sensor Data Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// Scripts/SensorFusion.cs\r\nusing UnityEngine;\r\nusing System.Collections.Generic;\r\n\r\npublic class SensorFusion : MonoBehaviour\r\n{\r\n    [Header("Sensor References")]\r\n    public LiDARSimulator lidar;\r\n    public DepthCameraSimulator depthCamera;\r\n    public IMUSimulator imu;\r\n\r\n    [Header("Fusion Parameters")]\r\n    public float lidarWeight = 0.4f;\r\n    public float depthWeight = 0.3f;\r\n    public float imuWeight = 0.3f;\r\n\r\n    private Queue<IMUReading> imuBuffer;\r\n    private float bufferDuration = 1.0f; // seconds\r\n\r\n    void Start()\r\n    {\r\n        imuBuffer = new Queue<IMUReading>();\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        // Integrate sensor data\r\n        ProcessSensorData();\r\n    }\r\n\r\n    void ProcessSensorData()\r\n    {\r\n        // Get current readings from all sensors\r\n        float[] lidarData = lidar != null ? lidar.GetRanges() : new float[0];\r\n        float[,] depthData = depthCamera != null ? depthCamera.GetDepthData() : new float[0, 0];\r\n        IMUReading imuData = imu != null ? imu.GetIMUReading() : new IMUReading();\r\n\r\n        // Store IMU data in buffer for temporal fusion\r\n        if (imu != null)\r\n        {\r\n            imuBuffer.Enqueue(imuData);\r\n\r\n            // Remove old data from buffer\r\n            while (imuBuffer.Count > 0 &&\r\n                   Time.time - imuBuffer.Peek().timestamp > bufferDuration)\r\n            {\r\n                imuBuffer.Dequeue();\r\n            }\r\n        }\r\n\r\n        // Perform sensor fusion (simplified example)\r\n        PerformFusion(lidarData, depthData, imuData);\r\n    }\r\n\r\n    void PerformFusion(float[] lidarData, float[,] depthData, IMUReading imuData)\r\n    {\r\n        // Example fusion algorithm: combine position estimates\r\n        Vector3 positionEstimate = Vector3.zero;\r\n\r\n        // Calculate position from IMU integration\r\n        if (imuBuffer.Count > 1)\r\n        {\r\n            IMUReading first = imuBuffer.Peek();\r\n            IMUReading last = imuBuffer.ToArray()[imuBuffer.Count - 1];\r\n\r\n            float deltaTime = (float)(last.timestamp - first.timestamp);\r\n            if (deltaTime > 0)\r\n            {\r\n                Vector3 velocity = (last.linearAcceleration + first.linearAcceleration) * 0.5f * deltaTime;\r\n                positionEstimate += velocity * imuWeight;\r\n            }\r\n        }\r\n\r\n        // In a real implementation, you would combine this with LiDAR and depth data\r\n        // for more accurate position estimation\r\n\r\n        Debug.Log($"Fused position estimate: {positionEstimate}");\r\n    }\r\n\r\n    public Vector3 GetFusedPositionEstimate()\r\n    {\r\n        // Return the fused position estimate\r\n        return transform.position; // Simplified\r\n    }\r\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-for-sensor-simulation",children:"Best Practices for Sensor Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"realistic-noise-modeling",children:"Realistic Noise Modeling"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Characterize Real Sensors"}),": Understand the noise characteristics of your actual sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Include Bias and Drift"}),": Model systematic errors that accumulate over time"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate Against Reality"}),": Compare simulated and real sensor data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temperature Effects"}),": Consider how temperature affects sensor performance"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Efficient Raycasting"}),": Use optimized algorithms for LiDAR simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Level of Detail"}),": Reduce sensor resolution when performance is critical"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parallel Processing"}),": Use multi-threading for sensor simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Caching"}),": Cache results when possible for repeated queries"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ground Truth Comparison"}),": Compare simulated data to known ground truth"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Edge Case Testing"}),": Test with challenging scenarios (bright light, occlusions, etc.)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cross-Sensor Validation"}),": Verify consistency between different sensor modalities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temporal Consistency"}),": Ensure sensor data is consistent over time"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"integration-with-rosros-2",children:"Integration with ROS/ROS 2"}),"\n",(0,a.jsx)(n.h3,{id:"ros-message-publishing",children:"ROS Message Publishing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// Scripts/ROSPublisher.cs\r\nusing UnityEngine;\r\nusing Unity.Robotics.ROSTCPConnector;\r\nusing RosMessageTypes.Sensor;\r\nusing RosMessageTypes.Geometry;\r\n\r\npublic class ROSPublisher : MonoBehaviour\r\n{\r\n    private ROSConnection ros;\r\n\r\n    [Header("Topic Names")]\r\n    public string laserScanTopic = "scan";\r\n    public string pointCloudTopic = "point_cloud";\r\n    public string imuTopic = "imu/data";\r\n\r\n    [Header("Sensors")]\r\n    public LiDARSimulator lidar;\r\n    public DepthCameraSimulator depthCamera;\r\n    public IMUSimulator imu;\r\n\r\n    void Start()\r\n    {\r\n        ros = ROSConnection.instance;\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        PublishSensorData();\r\n    }\r\n\r\n    void PublishSensorData()\r\n    {\r\n        if (lidar != null)\r\n        {\r\n            PublishLaserScan();\r\n        }\r\n\r\n        if (depthCamera != null)\r\n        {\r\n            PublishPointCloud();\r\n        }\r\n\r\n        if (imu != null)\r\n        {\r\n            PublishIMU();\r\n        }\r\n    }\r\n\r\n    void PublishLaserScan()\r\n    {\r\n        var scanMsg = new sensor_msgs.LaserScanMsg();\r\n        scanMsg.header = new std_msgs.HeaderMsg();\r\n        scanMsg.header.stamp = new builtin_interfaces.TimeMsg();\r\n        scanMsg.header.frame_id = "lidar_frame";\r\n\r\n        float[] ranges = lidar.GetRanges();\r\n        scanMsg.ranges = new float[ranges.Length];\r\n        for (int i = 0; i < ranges.Length; i++)\r\n        {\r\n            scanMsg.ranges[i] = ranges[i];\r\n        }\r\n\r\n        scanMsg.angle_min = lidar.minAngle;\r\n        scanMsg.angle_max = lidar.maxAngle;\r\n        scanMsg.angle_increment = (lidar.maxAngle - lidar.minAngle) / (ranges.Length - 1);\r\n        scanMsg.range_min = 0.1f;\r\n        scanMsg.range_max = lidar.maxRange;\r\n\r\n        ros.Publish(laserScanTopic, scanMsg);\r\n    }\r\n\r\n    void PublishPointCloud()\r\n    {\r\n        // Publish point cloud from depth camera\r\n        // Implementation would convert depth data to PointCloud2 message\r\n    }\r\n\r\n    void PublishIMU()\r\n    {\r\n        IMUReading reading = imu.GetIMUReading();\r\n\r\n        var imuMsg = new sensor_msgs.ImuMsg();\r\n        imuMsg.header = new std_msgs.HeaderMsg();\r\n        imuMsg.header.stamp = new builtin_interfaces.TimeMsg();\r\n        imuMsg.header.frame_id = "imu_frame";\r\n\r\n        // Convert to ROS message format\r\n        imuMsg.linear_acceleration.x = reading.linearAcceleration.x;\r\n        imuMsg.linear_acceleration.y = reading.linearAcceleration.y;\r\n        imuMsg.linear_acceleration.z = reading.linearAcceleration.z;\r\n\r\n        imuMsg.angular_velocity.x = reading.angularVelocity.x;\r\n        imuMsg.angular_velocity.y = reading.angularVelocity.y;\r\n        imuMsg.angular_velocity.z = reading.angularVelocity.z;\r\n\r\n        // Convert Euler angles to quaternion\r\n        imuMsg.orientation = Unity.Robotics.ROSTCPConnector.MessageExtensions.To<geometry_msgs.QuaternionMsg>(\r\n            Quaternion.Euler(reading.orientation * Mathf.Rad2Deg)\r\n        );\r\n\r\n        ros.Publish(imuTopic, imuMsg);\r\n    }\r\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement a LiDAR sensor simulation with realistic noise characteristics"}),"\n",(0,a.jsx)(n.li,{children:"Create a depth camera simulator with point cloud generation"}),"\n",(0,a.jsx)(n.li,{children:"Develop an IMU simulator with bias drift and random walk"}),"\n",(0,a.jsx)(n.li,{children:"Integrate all sensors with ROS/ROS 2 message publishing"}),"\n",(0,a.jsx)(n.li,{children:"Validate sensor data against expected real-world behavior"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation is fundamental to humanoid robotics development, providing realistic data for algorithm development and testing. By accurately modeling sensor characteristics including noise, bias, and drift, you can create effective simulation environments that closely match real-world conditions. Proper integration with ROS/ROS 2 enables seamless transition from simulation to real hardware. In the next module, we'll explore NVIDIA Isaac Sim for photorealistic simulation and synthetic data generation."}),"\n",(0,a.jsx)(n.h2,{id:"learning-path-adjustment",children:"Learning Path Adjustment"}),"\n",(0,a.jsx)(n.p,{children:"Based on your experience level, you may want to focus on:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Beginner"}),": Focus on basic sensor simulation and visualization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intermediate"}),": Dive deeper into noise modeling and sensor fusion"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Advanced"}),": Explore advanced sensor physics and AI training applications"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var i=r(6540);const a={},t=i.createContext(a);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);